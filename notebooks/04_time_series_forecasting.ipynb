{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2146c6b5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Stock Forecasting using ARIMA, Prophet & LSTM\"\n",
    "week: 4\n",
    "author: \"Praveen Kumar\"\n",
    "date: 2025-10-07\n",
    "version: v1.0\n",
    "---\n",
    "\n",
    "# Week 4: Stock Forecasting using ARIMA, Prophet & LSTM\n",
    "\n",
    "This notebook demonstrates time series forecasting using three different approaches:\n",
    "1. **ARIMA** - Classical statistical approach\n",
    "2. **Prophet** - Modern additive model\n",
    "3. **LSTM** - Deep learning approach\n",
    "\n",
    "We'll compare their performance on stock price forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a94079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SEED = 42\n",
    "SAMPLE_MODE = True  # Use subset for faster execution\n",
    "DATA_PATH = \"data/synthetic/stock_prices.csv\"\n",
    "SYMBOL = \"AAPL\"  # Stock symbol to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series specific\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Prophet\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Prophet not available. Installing...\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available for LSTM modeling\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(SEED)\n",
    "if TF_AVAILABLE:\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f86a1",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load stock price data using yfinance with fallback options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760aff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_data(symbol=\"AAPL\", period=\"5y\", sample_mode=True):\n",
    "    \"\"\"Load stock data with fallback to synthetic data\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading {symbol} data...\")\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"No data returned\")\n",
    "            \n",
    "        # Convert to lowercase columns\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # Subset for sample mode (last 1000 days for faster execution)\n",
    "        if sample_mode and len(df) > 1000:\n",
    "            df = df.tail(1000)\n",
    "            \n",
    "        print(f\"‚úÖ Loaded {len(df)} days of {symbol} data\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {symbol}: {e}\")\n",
    "        return generate_synthetic_stock_data(sample_mode)\n",
    "\n",
    "def generate_synthetic_stock_data(sample_mode=True):\n",
    "    \"\"\"Generate synthetic stock price data\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    n_days = 1000 if sample_mode else 2000\n",
    "    dates = pd.date_range(start='2020-01-01', periods=n_days, freq='D')\n",
    "    \n",
    "    # Generate price series with realistic properties\n",
    "    returns = np.random.normal(0.0005, 0.02, n_days)  # Daily returns\n",
    "    prices = 100 * np.exp(np.cumsum(returns))  # Cumulative returns to prices\n",
    "    \n",
    "    # Add some trend\n",
    "    trend = np.linspace(0, 0.3, n_days)\n",
    "    prices *= (1 + trend)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'close': prices,\n",
    "        'open': prices * (1 + np.random.normal(0, 0.005, n_days)),\n",
    "        'high': prices * (1 + np.abs(np.random.normal(0, 0.01, n_days))),\n",
    "        'low': prices * (1 - np.abs(np.random.normal(0, 0.01, n_days))),\n",
    "        'volume': np.random.randint(1000000, 5000000, n_days)\n",
    "    }, index=dates)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(df)} days of synthetic data\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_stock_data(SYMBOL, period=\"5y\", sample_mode=SAMPLE_MODE)\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13cc7a8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Visualize the data and check for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price chart\n",
    "axes[0, 0].plot(df.index, df['close'], linewidth=1.5)\n",
    "axes[0, 0].set_title('Stock Price Over Time')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "returns = df['close'].pct_change().dropna()\n",
    "axes[0, 1].plot(df.index[1:], returns, linewidth=0.8)\n",
    "axes[0, 1].set_title('Daily Returns')\n",
    "axes[0, 1].set_ylabel('Returns')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns distribution\n",
    "axes[1, 0].hist(returns, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Returns Distribution')\n",
    "axes[1, 0].set_xlabel('Daily Returns')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling statistics\n",
    "window = 30\n",
    "rolling_mean = df['close'].rolling(window=window).mean()\n",
    "rolling_std = df['close'].rolling(window=window).std()\n",
    "\n",
    "axes[1, 1].plot(df.index, df['close'], label='Price', alpha=0.7)\n",
    "axes[1, 1].plot(df.index, rolling_mean, label=f'{window}-day Mean', linewidth=2)\n",
    "axes[1, 1].fill_between(df.index, \n",
    "                       rolling_mean - 2*rolling_std, \n",
    "                       rolling_mean + 2*rolling_std, \n",
    "                       alpha=0.2, label='¬±2 Std')\n",
    "axes[1, 1].set_title(f'Price with {window}-day Rolling Statistics')\n",
    "axes[1, 1].set_ylabel('Price')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Price Statistics:\")\n",
    "print(df['close'].describe())\n",
    "print(f\"\\nReturns Statistics:\")\n",
    "print(returns.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256eaef",
   "metadata": {},
   "source": [
    "## Stationarity Testing\n",
    "\n",
    "Check if the series is stationary using the Augmented Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(series, name=\"Series\"):\n",
    "    \"\"\"Perform ADF test and display results\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    \n",
    "    print(f'Stationarity Test Results for {name}:')\n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    # Interpretation\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"‚úÖ Series is stationary (reject null hypothesis)\")\n",
    "        is_stationary = True\n",
    "    else:\n",
    "        print(\"‚ùå Series is non-stationary (fail to reject null hypothesis)\")\n",
    "        is_stationary = False\n",
    "    \n",
    "    return is_stationary, result[1]\n",
    "\n",
    "# Test stationarity for prices and returns\n",
    "print(\"=\" * 60)\n",
    "is_stationary_price, p_val_price = check_stationarity(df['close'], \"Stock Prices\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "is_stationary_returns, p_val_returns = check_stationarity(returns, \"Daily Returns\")\n",
    "\n",
    "# Choose target variable based on stationarity\n",
    "if is_stationary_returns:\n",
    "    target_series = returns\n",
    "    target_name = \"Daily Returns\"\n",
    "    print(f\"\\nüìä Using {target_name} for modeling (stationary)\")\n",
    "else:\n",
    "    # Use differenced prices if returns are not stationary\n",
    "    target_series = df['close'].diff().dropna()\n",
    "    target_name = \"Price Differences\"\n",
    "    print(f\"\\nüìä Using {target_name} for modeling\")\n",
    "\n",
    "print(f\"Target series length: {len(target_series)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fc12b",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Split data into train/test sets for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "split_point = int(len(target_series) * 0.8)\n",
    "train_data = target_series[:split_point].copy()\n",
    "test_data = target_series[split_point:].copy()\n",
    "\n",
    "print(f\"Data Split:\")\n",
    "print(f\"Training set: {len(train_data)} observations\")\n",
    "print(f\"Test set: {len(test_data)} observations\")\n",
    "print(f\"Split date: {train_data.index[-1].date()}\")\n",
    "\n",
    "# Visualization of split\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data.index, train_data, label='Training Data', color='blue', alpha=0.7)\n",
    "plt.plot(test_data.index, test_data, label='Test Data', color='red', alpha=0.7)\n",
    "plt.axvline(x=train_data.index[-1], color='black', linestyle='--', alpha=0.5, label='Split Point')\n",
    "plt.title(f'{target_name} - Train/Test Split')\n",
    "plt.ylabel(target_name)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdea682",
   "metadata": {},
   "source": [
    "## Model 1: ARIMA\n",
    "\n",
    "Implement ARIMA forecasting with automatic parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arima_model(train_data, max_p=3, max_q=3):\n",
    "    \"\"\"Build ARIMA model with automatic parameter selection\"\"\"\n",
    "    \n",
    "    # Since we're using returns (already differenced), d=0\n",
    "    # If using prices, you might need d=1\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(\"Searching for optimal ARIMA parameters...\")\n",
    "    \n",
    "    # Grid search for p and q (d=0 for stationary series)\n",
    "    for p in range(max_p + 1):\n",
    "        for q in range(max_q + 1):\n",
    "            try:\n",
    "                model = ARIMA(train_data, order=(p, 0, q))\n",
    "                fitted_model = model.fit()\n",
    "                \n",
    "                if fitted_model.aic < best_aic:\n",
    "                    best_aic = fitted_model.aic\n",
    "                    best_order = (p, 0, q)\n",
    "                    best_model = fitted_model\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    print(f\"Best ARIMA order: {best_order}\")\n",
    "    print(f\"Best AIC: {best_aic:.4f}\")\n",
    "    \n",
    "    return best_model, best_order\n",
    "\n",
    "# Build ARIMA model\n",
    "arima_model, arima_order = build_arima_model(train_data)\n",
    "\n",
    "# Generate forecasts\n",
    "forecast_steps = len(test_data)\n",
    "arima_forecast = arima_model.forecast(steps=forecast_steps)\n",
    "arima_conf_int = arima_model.get_forecast(steps=forecast_steps).conf_int()\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nARIMA Model Summary:\")\n",
    "print(arima_model.summary())\n",
    "\n",
    "# Calculate ARIMA metrics\n",
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"Calculate forecast accuracy metrics\"\"\"\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "\n",
    "arima_metrics = calculate_metrics(test_data, arima_forecast)\n",
    "print(f\"\\nARIMA Forecast Metrics:\")\n",
    "for metric, value in arima_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d650e",
   "metadata": {},
   "source": [
    "## Model 2: Prophet\n",
    "\n",
    "Implement Prophet forecasting with automatic seasonality detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a451850",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROPHET_AVAILABLE:\n",
    "    def build_prophet_model(train_data):\n",
    "        \"\"\"Build Prophet model\"\"\"\n",
    "        # Prepare data in Prophet format\n",
    "        prophet_df = pd.DataFrame({\n",
    "            'ds': train_data.index,\n",
    "            'y': train_data.values\n",
    "        })\n",
    "        \n",
    "        # Create and fit Prophet model\n",
    "        model = Prophet(\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=False,  # Not enough data typically\n",
    "            changepoint_prior_scale=0.05  # Flexibility of trend\n",
    "        )\n",
    "        \n",
    "        model.fit(prophet_df)\n",
    "        return model\n",
    "    \n",
    "    # Build Prophet model\n",
    "    print(\"Building Prophet model...\")\n",
    "    prophet_model = build_prophet_model(train_data)\n",
    "    \n",
    "    # Create future dataframe for forecasting\n",
    "    future_dates = pd.DataFrame({\n",
    "        'ds': test_data.index\n",
    "    })\n",
    "    \n",
    "    # Generate forecasts\n",
    "    prophet_forecast = prophet_model.predict(future_dates)\n",
    "    prophet_predictions = prophet_forecast['yhat'].values\n",
    "    \n",
    "    # Calculate Prophet metrics\n",
    "    prophet_metrics = calculate_metrics(test_data, prophet_predictions)\n",
    "    print(f\"\\nProphet Forecast Metrics:\")\n",
    "    for metric, value in prophet_metrics.items():\n",
    "        print(f\"{metric}: {value:.6f}\")\n",
    "    \n",
    "    # Plot Prophet components\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Forecast plot\n",
    "    prophet_model.plot(prophet_forecast, ax=axes[0])\n",
    "    axes[0].set_title('Prophet Forecast')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Components plot\n",
    "    prophet_model.plot_components(prophet_forecast, ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Prophet not available. Skipping Prophet modeling.\")\n",
    "    prophet_predictions = np.zeros(len(test_data))  # Dummy values\n",
    "    prophet_metrics = {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc3c12",
   "metadata": {},
   "source": [
    "## Model 3: LSTM\n",
    "\n",
    "Implement LSTM neural network for sequence forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    def create_lstm_dataset(data, lookback=20):\n",
    "        \"\"\"Create supervised learning dataset for LSTM\"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(lookback, len(data)):\n",
    "            X.append(data[i-lookback:i])\n",
    "            y.append(data[i])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_lstm_model(train_data, lookback=20, epochs=50):\n",
    "        \"\"\"Build and train LSTM model\"\"\"\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create supervised learning dataset\n",
    "        X_train, y_train = create_lstm_dataset(train_scaled, lookback)\n",
    "        \n",
    "        # Reshape for LSTM [samples, time steps, features]\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "        \n",
    "        # Build LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=(lookback, 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"Training LSTM model for {epochs} epochs...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0,\n",
    "            shuffle=False  # Important for time series\n",
    "        )\n",
    "        \n",
    "        return model, scaler, history\n",
    "    \n",
    "    # Build LSTM model\n",
    "    LOOKBACK = 20\n",
    "    lstm_model, lstm_scaler, lstm_history = build_lstm_model(train_data, LOOKBACK, epochs=30)\n",
    "    \n",
    "    # Prepare test data for LSTM prediction\n",
    "    # We need the last LOOKBACK points from training data\n",
    "    full_data = pd.concat([train_data, test_data])\n",
    "    full_scaled = lstm_scaler.transform(full_data.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Generate predictions for test set\n",
    "    lstm_predictions = []\n",
    "    \n",
    "    for i in range(len(train_data), len(full_data)):\n",
    "        # Get the last LOOKBACK points\n",
    "        X_test = full_scaled[i-LOOKBACK:i].reshape(1, LOOKBACK, 1)\n",
    "        \n",
    "        # Predict next value\n",
    "        pred_scaled = lstm_model.predict(X_test, verbose=0)\n",
    "        pred = lstm_scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]\n",
    "        lstm_predictions.append(pred)\n",
    "    \n",
    "    lstm_predictions = np.array(lstm_predictions)\n",
    "    \n",
    "    # Calculate LSTM metrics\n",
    "    lstm_metrics = calculate_metrics(test_data, lstm_predictions)\n",
    "    print(f\"\\nLSTM Forecast Metrics:\")\n",
    "    for metric, value in lstm_metrics.items():\n",
    "        print(f\"{metric}: {value:.6f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(lstm_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('LSTM Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_data.index, test_data.values, label='Actual', linewidth=2)\n",
    "    plt.plot(test_data.index, lstm_predictions, label='LSTM Prediction', linewidth=2)\n",
    "    plt.title('LSTM Forecast vs Actual')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå TensorFlow not available. Skipping LSTM modeling.\")\n",
    "    lstm_predictions = np.zeros(len(test_data))  # Dummy values\n",
    "    lstm_metrics = {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e06720",
   "metadata": {},
   "source": [
    "## Model Comparison & Evaluation\n",
    "\n",
    "Compare all three models and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "models = ['ARIMA', 'Prophet', 'LSTM']\n",
    "predictions = [arima_forecast, prophet_predictions, lstm_predictions]\n",
    "metrics = [arima_metrics, prophet_metrics, lstm_metrics]\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Forecast comparison\n",
    "axes[0, 0].plot(test_data.index, test_data.values, label='Actual', linewidth=3, color='black')\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i, (model, pred, color) in enumerate(zip(models, predictions, colors)):\n",
    "    if not np.isnan(pred).all():  # Only plot if model was actually trained\n",
    "        axes[0, 0].plot(test_data.index, pred, label=f'{model} Forecast', \n",
    "                       linewidth=2, color=color, alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_title('Forecast Comparison')\n",
    "axes[0, 0].set_ylabel(target_name)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add confidence intervals for ARIMA\n",
    "if 'arima_conf_int' in locals():\n",
    "    axes[0, 0].fill_between(test_data.index, \n",
    "                           arima_conf_int.iloc[:, 0], \n",
    "                           arima_conf_int.iloc[:, 1], \n",
    "                           alpha=0.2, color='blue', label='ARIMA Conf. Int.')\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_df = pd.DataFrame(metrics, index=models)\n",
    "metrics_df = metrics_df.dropna()  # Remove models that weren't trained\n",
    "\n",
    "for i, metric in enumerate(['RMSE', 'MAE', 'MAPE']):\n",
    "    ax = axes[0, 1] if i == 0 else axes[1, i-1]\n",
    "    \n",
    "    if not metrics_df.empty and metric in metrics_df.columns:\n",
    "        bars = ax.bar(metrics_df.index, metrics_df[metric], \n",
    "                     color=['blue', 'green', 'red'][:len(metrics_df)], alpha=0.7)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, metrics_df[metric]):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metrics_df[metric])*0.01, \n",
    "                   f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Residuals analysis\n",
    "residuals_data = []\n",
    "for model, pred in zip(models, predictions):\n",
    "    if not np.isnan(pred).all():\n",
    "        residuals = test_data.values - pred\n",
    "        residuals_data.append(residuals)\n",
    "\n",
    "if residuals_data:\n",
    "    axes[1, 2].boxplot(residuals_data, labels=[models[i] for i in range(len(residuals_data))])\n",
    "    axes[1, 2].set_title('Residuals Distribution')\n",
    "    axes[1, 2].set_ylabel('Residuals')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for model, metric in zip(models, metrics):\n",
    "    print(f\"\\n{model}:\")\n",
    "    if not any(np.isnan(list(metric.values()))):\n",
    "        for k, v in metric.items():\n",
    "            print(f\"  {k}: {v:.6f}\")\n",
    "        \n",
    "        if metric['RMSE'] < best_rmse:\n",
    "            best_rmse = metric['RMSE']\n",
    "            best_model = model\n",
    "    else:\n",
    "        print(\"  Model not available/trained\")\n",
    "\n",
    "if best_model:\n",
    "    print(f\"\\nüèÜ Best performing model: {best_model} (RMSE: {best_rmse:.6f})\")\n",
    "\n",
    "# Save forecasts\n",
    "forecast_results = pd.DataFrame({\n",
    "    'Date': test_data.index,\n",
    "    'Actual': test_data.values,\n",
    "    'ARIMA': predictions[0],\n",
    "    'Prophet': predictions[1],\n",
    "    'LSTM': predictions[2]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '/tmp/week4_forecasts.csv'  # Use /tmp for cross-platform compatibility\n",
    "forecast_results.to_csv(output_path, index=False)\n",
    "print(f\"\\nüíæ Forecasts saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eea448",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Complete the following exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Exponential Smoothing Baseline\n",
    "**TODO**: Add Holt-Winters exponential smoothing as a baseline model and compare its performance.\n",
    "\n",
    "### Exercise 2: Walk-forward Validation  \n",
    "**TODO**: Implement walk-forward validation for ARIMA model to assess performance over multiple forecast periods.\n",
    "\n",
    "### Exercise 3: LSTM Hyperparameter Tuning\n",
    "**TODO**: Experiment with different LSTM lookback windows (10, 30, 50 days) and compare results.\n",
    "\n",
    "*Implementation details and solutions are provided in the instructor notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37748b",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "### Key Findings:\n",
    "1. **Model Performance**: Each model has different strengths based on data characteristics\n",
    "2. **Stationarity**: Important for ARIMA; Prophet and LSTM handle non-stationary data better  \n",
    "3. **Seasonality**: Prophet excels with seasonal patterns; LSTM can learn complex patterns\n",
    "4. **Data Requirements**: LSTM needs more data; ARIMA works with smaller datasets\n",
    "\n",
    "### Practical Recommendations:\n",
    "- Use **ARIMA** for short-term, linear forecasts with limited data\n",
    "- Use **Prophet** for business forecasting with seasonal patterns\n",
    "- Use **LSTM** for complex, non-linear patterns with sufficient data\n",
    "- Consider **ensemble methods** combining multiple approaches\n",
    "\n",
    "### Next Steps:\n",
    "- Explore ensemble forecasting methods\n",
    "- Implement real-time model updating\n",
    "- Add exogenous variables (economic indicators)\n",
    "- Develop automated model selection pipelines"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
