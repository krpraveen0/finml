{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c55399",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Week 02 — Data Wrangling & Feature Engineering for Finance\"\n",
    "week: 2\n",
    "author: \"Praveen Kumar\"\n",
    "date: 2025-10-07\n",
    "duration: \"3 hours\"\n",
    "prerequisites: [\"Week 1: Financial Modelling & ML Basics\"]\n",
    "tags: [\"feature-engineering\",\"data-preprocessing\",\"finance\",\"PCA\"]\n",
    "version: v1.0\n",
    "---\n",
    "\n",
    "# Week 02 — Feature Engineering & Selection for Financial Data\n",
    "\n",
    "## Student Notebook: Complete Feature Engineering Pipeline\n",
    "\n",
    "This notebook demonstrates the end-to-end process of cleaning financial data, engineering meaningful features, and selecting the most predictive variables for machine learning models.\n",
    "\n",
    "### Learning Goals:\n",
    "- Master financial data preprocessing techniques\n",
    "- Create technical indicators and derived features\n",
    "- Apply feature selection methods (correlation, PCA, importance)\n",
    "- Build a reproducible feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and Configuration\n",
    "SEED = 42\n",
    "SAMPLE_MODE = True  # Set to True for quick runs, False for comprehensive analysis\n",
    "DATA_PATH = \"data/synthetic/stock_data.csv\"\n",
    "TICKER = \"AAPL\"  # Default ticker for download\n",
    "START_DATE = \"2022-01-01\"\n",
    "END_DATE = \"2024-01-01\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"SAMPLE_MODE: {SAMPLE_MODE}\")\n",
    "print(f\"DATA_PATH: {DATA_PATH}\")\n",
    "print(f\"TICKER: {TICKER}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check and install technical analysis library\n",
    "try:\n",
    "    import talib\n",
    "    print(\"TA-Lib available\")\n",
    "except ImportError:\n",
    "    print(\"TA-Lib not available, will use manual calculations\")\n",
    "\n",
    "# Install yfinance if needed\n",
    "try:\n",
    "    import yfinance\n",
    "    print(\"yfinance already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing yfinance...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"yfinance\", \"-q\"])\n",
    "    print(\"yfinance installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Financial Data\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YF_AVAILABLE = False\n",
    "    print(\"Warning: yfinance not available. Will use synthetic data only.\")\n",
    "\n",
    "# Set random seed and configure\n",
    "np.random.seed(SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Setup Complete!\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn: {sns.__version__}\")\n",
    "print(f\"YFinance available: {YF_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917da8cd",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Initial Assessment\n",
    "\n",
    "### Data Loading Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Check Local Data] --> B{File Exists?}\n",
    "    B -->|Yes| C[Load CSV]\n",
    "    B -->|No| D[Download from yfinance]\n",
    "    C --> E[Data Quality Check]\n",
    "    D --> E\n",
    "    E --> F[Clean Dataset]\n",
    "```\n",
    "\n",
    "We'll implement a robust data loading strategy that handles both local files and live data downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16803e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Function with Fallback Logic\n",
    "def load_financial_data():\n",
    "    \"\"\"Load financial data with multiple fallback strategies.\"\"\"\n",
    "    \n",
    "    # Strategy 1: Try local synthetic data\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        try:\n",
    "            print(f\"Loading data from {DATA_PATH}\")\n",
    "            data = pd.read_csv(DATA_PATH, index_col=0, parse_dates=True)\n",
    "            print(f\"Successfully loaded local data: {data.shape}\")\n",
    "            return data, \"local\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load local data: {e}\")\n",
    "    \n",
    "    # Strategy 2: Download from yfinance\n",
    "    if YF_AVAILABLE:\n",
    "        try:\n",
    "            print(f\"Downloading {TICKER} data from Yahoo Finance...\")\n",
    "            if SAMPLE_MODE:\n",
    "                # Smaller dataset for quick processing\n",
    "                data = yf.download(TICKER, start=\"2022-01-01\", end=\"2023-12-31\", progress=False)\n",
    "            else:\n",
    "                # Full dataset\n",
    "                data = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
    "            \n",
    "            print(f\"Successfully downloaded data: {data.shape}\")\n",
    "            return data.dropna(), \"yfinance\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download data: {e}\")\n",
    "    \n",
    "    # Strategy 3: Generate synthetic data\n",
    "    print(\"Generating synthetic financial data...\")\n",
    "    dates = pd.date_range(start=\"2022-01-01\", end=\"2023-12-31\", freq='D')\n",
    "    dates = dates[dates.weekday < 5]  # Remove weekends\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Generate realistic price series with trend and volatility\n",
    "    initial_price = 150.0\n",
    "    returns = np.random.normal(0.0005, 0.02, n_days)  # Daily returns\n",
    "    returns[0] = 0  # First return is zero\n",
    "    \n",
    "    # Add some market regime changes\n",
    "    volatility_regimes = np.random.choice([1, 1.5, 0.7], n_days, p=[0.7, 0.15, 0.15])\n",
    "    returns = returns * volatility_regimes\n",
    "    \n",
    "    # Calculate prices\n",
    "    prices = initial_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Generate OHLC data\n",
    "    noise_factor = 0.003\n",
    "    data = pd.DataFrame({\n",
    "        'Open': prices * (1 + np.random.normal(0, noise_factor, n_days)),\n",
    "        'High': prices * (1 + np.abs(np.random.normal(0, noise_factor * 1.5, n_days))),\n",
    "        'Low': prices * (1 - np.abs(np.random.normal(0, noise_factor * 1.5, n_days))),\n",
    "        'Close': prices,\n",
    "        'Volume': np.random.randint(50000000, 200000000, n_days),\n",
    "        'Adj Close': prices\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Ensure High >= Low and both contain Open/Close\n",
    "    data['High'] = np.maximum(data['High'], np.maximum(data['Open'], data['Close']))\n",
    "    data['Low'] = np.minimum(data['Low'], np.minimum(data['Open'], data['Close']))\n",
    "    \n",
    "    print(f\"Generated synthetic data: {data.shape}\")\n",
    "    return data, \"synthetic\"\n",
    "\n",
    "# Load the data\n",
    "stock_data, data_source = load_financial_data()\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"Source: {data_source}\")\n",
    "print(f\"Shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data.index.min().date()} to {stock_data.index.max().date()}\")\n",
    "print(f\"Columns: {list(stock_data.columns)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "stock_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d5e9d",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning and Preprocessing\n",
    "\n",
    "### Data Quality Assessment\n",
    "\n",
    "Let's examine our data for common issues:\n",
    "- Missing values\n",
    "- Outliers and anomalies  \n",
    "- Data consistency checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment and Cleaning\n",
    "def assess_data_quality(data):\n",
    "    \"\"\"Comprehensive data quality assessment.\"\"\"\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    missing_counts = data.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(data)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Count'] > 0])\n",
    "    \n",
    "    # Check for duplicated index values\n",
    "    print(f\"\\n2. Duplicate Dates: {data.index.duplicated().sum()}\")\n",
    "    \n",
    "    # Check for zero/negative prices\n",
    "    price_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "    available_price_cols = [col for col in price_cols if col in data.columns]\n",
    "    \n",
    "    print(f\"\\n3. Zero/Negative Prices:\")\n",
    "    for col in available_price_cols:\n",
    "        zero_count = (data[col] <= 0).sum()\n",
    "        if zero_count > 0:\n",
    "            print(f\"   {col}: {zero_count} zero/negative values\")\n",
    "    \n",
    "    # Check for outliers using IQR method\n",
    "    print(f\"\\n4. Outlier Analysis (IQR Method):\")\n",
    "    for col in available_price_cols:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\n",
    "        print(f\"   {col}: {outliers} outliers ({outliers/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "def clean_financial_data(data):\n",
    "    \"\"\"Clean financial data using standard techniques.\"\"\"\n",
    "    print(\"\\n=== DATA CLEANING ===\")\n",
    "    cleaned_data = data.copy()\n",
    "    \n",
    "    # Remove any duplicate dates\n",
    "    if cleaned_data.index.duplicated().sum() > 0:\n",
    "        print(f\"Removing {cleaned_data.index.duplicated().sum()} duplicate dates\")\n",
    "        cleaned_data = cleaned_data[~cleaned_data.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = cleaned_data.isnull().sum().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"Filling {missing_before} missing values using forward fill\")\n",
    "        cleaned_data = cleaned_data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Winsorize outliers (cap at 1st and 99th percentiles)\n",
    "    price_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "    available_cols = [col for col in price_columns if col in cleaned_data.columns]\n",
    "    \n",
    "    outliers_treated = 0\n",
    "    for col in available_cols:\n",
    "        lower_cap = cleaned_data[col].quantile(0.01)\n",
    "        upper_cap = cleaned_data[col].quantile(0.99)\n",
    "        \n",
    "        outliers_count = ((cleaned_data[col] < lower_cap) | (cleaned_data[col] > upper_cap)).sum()\n",
    "        if outliers_count > 0:\n",
    "            cleaned_data[col] = cleaned_data[col].clip(lower=lower_cap, upper=upper_cap)\n",
    "            outliers_treated += outliers_count\n",
    "            print(f\"Winsorized {outliers_count} outliers in {col}\")\n",
    "    \n",
    "    print(f\"Total outliers treated: {outliers_treated}\")\n",
    "    print(f\"Cleaned dataset shape: {cleaned_data.shape}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Assess and clean the data\n",
    "quality_info = assess_data_quality(stock_data)\n",
    "cleaned_data = clean_financial_data(stock_data)\n",
    "\n",
    "# Visualize the cleaned data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price series\n",
    "axes[0, 0].plot(cleaned_data.index, cleaned_data['Close'], linewidth=1)\n",
    "axes[0, 0].set_title('Stock Price Over Time')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume series\n",
    "if 'Volume' in cleaned_data.columns:\n",
    "    axes[0, 1].plot(cleaned_data.index, cleaned_data['Volume'], color='orange', linewidth=1)\n",
    "    axes[0, 1].set_title('Trading Volume Over Time')\n",
    "    axes[0, 1].set_ylabel('Volume')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily price change distribution\n",
    "price_changes = cleaned_data['Close'].pct_change().dropna()\n",
    "axes[1, 0].hist(price_changes, bins=50, alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Daily Return Distribution')\n",
    "axes[1, 0].set_xlabel('Daily Return')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of price columns\n",
    "price_cols = [col for col in ['Open', 'High', 'Low', 'Close'] if col in cleaned_data.columns]\n",
    "if price_cols:\n",
    "    cleaned_data[price_cols].boxplot(ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Price Distribution (OHLC)')\n",
    "    axes[1, 1].set_ylabel('Price ($)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData cleaning completed. Ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1511738",
   "metadata": {},
   "source": [
    "## Section 3: Feature Engineering Pipeline\n",
    "\n",
    "### Feature Engineering Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Clean Data] --> B[Basic Features]\n",
    "    B --> C[Technical Indicators]\n",
    "    C --> D[Volatility Features]\n",
    "    D --> E[Momentum Features]\n",
    "    E --> F[Lag Features]\n",
    "    F --> G[Feature Matrix]\n",
    "    \n",
    "    B --> B1[Returns, Log Returns]\n",
    "    C --> C1[MA, RSI, MACD]\n",
    "    D --> D1[Rolling Vol, GARCH]\n",
    "    E --> E1[Price Momentum, ROC]\n",
    "    F --> F1[Lagged Returns, Prices]\n",
    "```\n",
    "\n",
    "Let's systematically create a comprehensive set of financial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a84a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Feature Engineering Functions\n",
    "\n",
    "def calculate_returns(data, price_col='Close'):\n",
    "    \"\"\"Calculate various types of returns.\"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Simple returns\n",
    "    features['simple_return'] = data[price_col].pct_change()\n",
    "    \n",
    "    # Log returns (preferred for modeling)\n",
    "    features['log_return'] = np.log(data[price_col] / data[price_col].shift(1))\n",
    "    \n",
    "    # Multi-period returns\n",
    "    for period in [5, 10, 20]:\n",
    "        features[f'return_{period}d'] = data[price_col].pct_change(period)\n",
    "        features[f'log_return_{period}d'] = np.log(data[price_col] / data[price_col].shift(period))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_moving_averages(data, price_col='Close'):\n",
    "    \"\"\"Calculate various moving averages.\"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Simple Moving Averages\n",
    "    for window in [10, 20, 50]:\n",
    "        features[f'SMA_{window}'] = data[price_col].rolling(window=window).mean()\n",
    "        features[f'price_to_SMA_{window}'] = data[price_col] / features[f'SMA_{window}']\n",
    "    \n",
    "    # Exponential Moving Averages\n",
    "    for window in [10, 20]:\n",
    "        features[f'EMA_{window}'] = data[price_col].ewm(span=window).mean()\n",
    "        features[f'price_to_EMA_{window}'] = data[price_col] / features[f'EMA_{window}']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_volatility_features(data, price_col='Close'):\n",
    "    \"\"\"Calculate volatility-based features.\"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Returns for volatility calculation\n",
    "    returns = data[price_col].pct_change()\n",
    "    \n",
    "    # Rolling volatility (different windows)\n",
    "    for window in [5, 10, 20]:\n",
    "        features[f'volatility_{window}d'] = returns.rolling(window=window).std() * np.sqrt(252)\n",
    "        features[f'volatility_{window}d_norm'] = features[f'volatility_{window}d'] / features[f'volatility_{window}d'].rolling(60).mean()\n",
    "    \n",
    "    # Price range features\n",
    "    if all(col in data.columns for col in ['High', 'Low']):\n",
    "        features['daily_range'] = (data['High'] - data['Low']) / data[price_col]\n",
    "        features['overnight_gap'] = (data['Open'] - data[price_col].shift(1)) / data[price_col].shift(1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index manually.\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD manually.\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    \n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def calculate_bollinger_bands(prices, window=20, num_std=2):\n",
    "    \"\"\"Calculate Bollinger Bands.\"\"\"\n",
    "    sma = prices.rolling(window=window).mean()\n",
    "    std = prices.rolling(window=window).std()\n",
    "    upper_band = sma + (std * num_std)\n",
    "    lower_band = sma - (std * num_std)\n",
    "    \n",
    "    return upper_band, lower_band, sma\n",
    "\n",
    "def calculate_technical_indicators(data, price_col='Close'):\n",
    "    \"\"\"Calculate comprehensive technical indicators.\"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    prices = data[price_col]\n",
    "    \n",
    "    # RSI\n",
    "    features['RSI'] = calculate_rsi(prices)\n",
    "    features['RSI_overbought'] = (features['RSI'] > 70).astype(int)\n",
    "    features['RSI_oversold'] = (features['RSI'] < 30).astype(int)\n",
    "    \n",
    "    # MACD\n",
    "    macd_line, signal_line, histogram = calculate_macd(prices)\n",
    "    features['MACD'] = macd_line\n",
    "    features['MACD_signal'] = signal_line\n",
    "    features['MACD_histogram'] = histogram\n",
    "    features['MACD_bullish'] = (features['MACD'] > features['MACD_signal']).astype(int)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb_upper, bb_lower, bb_middle = calculate_bollinger_bands(prices)\n",
    "    features['BB_upper'] = bb_upper\n",
    "    features['BB_lower'] = bb_lower\n",
    "    features['BB_middle'] = bb_middle\n",
    "    features['BB_width'] = (bb_upper - bb_lower) / bb_middle\n",
    "    features['BB_position'] = (prices - bb_lower) / (bb_upper - bb_lower)\n",
    "    \n",
    "    # Price momentum\n",
    "    features['momentum_5'] = prices / prices.shift(5) - 1\n",
    "    features['momentum_10'] = prices / prices.shift(10) - 1\n",
    "    features['momentum_20'] = prices / prices.shift(20) - 1\n",
    "    \n",
    "    # Rate of Change\n",
    "    features['ROC_5'] = ((prices - prices.shift(5)) / prices.shift(5)) * 100\n",
    "    features['ROC_10'] = ((prices - prices.shift(10)) / prices.shift(10)) * 100\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_lag_features(data, price_col='Close', max_lags=5):\n",
    "    \"\"\"Calculate lagged features.\"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Lagged returns\n",
    "    returns = data[price_col].pct_change()\n",
    "    for lag in range(1, max_lags + 1):\n",
    "        features[f'return_lag_{lag}'] = returns.shift(lag)\n",
    "    \n",
    "    # Lagged prices (normalized)\n",
    "    for lag in range(1, max_lags + 1):\n",
    "        features[f'price_lag_{lag}'] = data[price_col].shift(lag) / data[price_col]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply all feature engineering steps\n",
    "print(\"Starting comprehensive feature engineering...\")\n",
    "\n",
    "# Step 1: Basic return features\n",
    "print(\"1. Calculating returns...\")\n",
    "return_features = calculate_returns(cleaned_data)\n",
    "\n",
    "# Step 2: Moving averages\n",
    "print(\"2. Calculating moving averages...\")\n",
    "ma_features = calculate_moving_averages(cleaned_data)\n",
    "\n",
    "# Step 3: Volatility features\n",
    "print(\"3. Calculating volatility features...\")\n",
    "vol_features = calculate_volatility_features(cleaned_data)\n",
    "\n",
    "# Step 4: Technical indicators\n",
    "print(\"4. Calculating technical indicators...\")\n",
    "tech_features = calculate_technical_indicators(cleaned_data)\n",
    "\n",
    "# Step 5: Lag features\n",
    "print(\"5. Calculating lag features...\")\n",
    "lag_features = calculate_lag_features(cleaned_data)\n",
    "\n",
    "# Combine all features\n",
    "feature_matrix = pd.concat([\n",
    "    return_features,\n",
    "    ma_features, \n",
    "    vol_features,\n",
    "    tech_features,\n",
    "    lag_features\n",
    "], axis=1)\n",
    "\n",
    "# Add target variable (next day return)\n",
    "feature_matrix['target'] = return_features['simple_return'].shift(-1)\n",
    "\n",
    "# Remove rows with missing values\n",
    "initial_shape = feature_matrix.shape\n",
    "feature_matrix = feature_matrix.dropna()\n",
    "final_shape = feature_matrix.shape\n",
    "\n",
    "print(f\"\\nFeature Engineering Complete!\")\n",
    "print(f\"Initial shape: {initial_shape}\")\n",
    "print(f\"Final shape: {final_shape}\")\n",
    "print(f\"Total features created: {final_shape[1] - 1}\")  # Excluding target\n",
    "print(f\"Date range: {feature_matrix.index.min().date()} to {feature_matrix.index.max().date()}\")\n",
    "\n",
    "# Display feature summary\n",
    "print(f\"\\nFeature Categories:\")\n",
    "feature_names = feature_matrix.columns.tolist()\n",
    "feature_names.remove('target')\n",
    "\n",
    "categories = {\n",
    "    'Returns': [f for f in feature_names if 'return' in f and 'lag' not in f],\n",
    "    'Moving Averages': [f for f in feature_names if any(x in f for x in ['SMA', 'EMA', 'price_to'])],\n",
    "    'Volatility': [f for f in feature_names if 'volatility' in f or 'range' in f or 'gap' in f],\n",
    "    'Technical Indicators': [f for f in feature_names if any(x in f for x in ['RSI', 'MACD', 'BB', 'momentum', 'ROC'])],\n",
    "    'Lag Features': [f for f in feature_names if 'lag' in f]\n",
    "}\n",
    "\n",
    "for category, features in categories.items():\n",
    "    print(f\"  {category}: {len(features)} features\")\n",
    "\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784043c",
   "metadata": {},
   "source": [
    "## Section 4: Feature Correlation Analysis\n",
    "\n",
    "Understanding relationships between features is crucial for model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3171af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Correlation Analysis\n",
    "def analyze_feature_correlations(data, target_col='target', threshold=0.8):\n",
    "    \"\"\"Analyze correlations between features and with target.\"\"\"\n",
    "    \n",
    "    features = data.drop(columns=[target_col])\n",
    "    target = data[target_col]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = features.corr()\n",
    "    \n",
    "    # Find highly correlated feature pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = abs(corr_matrix.iloc[i, j])\n",
    "            if corr_val > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature1': corr_matrix.columns[i],\n",
    "                    'Feature2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    # Calculate correlations with target\n",
    "    target_corrs = features.corrwith(target).abs().sort_values(ascending=False)\n",
    "    \n",
    "    return corr_matrix, high_corr_pairs, target_corrs\n",
    "\n",
    "# Perform correlation analysis\n",
    "print(\"Analyzing feature correlations...\")\n",
    "corr_matrix, high_corr_pairs, target_correlations = analyze_feature_correlations(feature_matrix)\n",
    "\n",
    "# Display high correlation pairs\n",
    "print(f\"\\nHigh Correlation Pairs (|correlation| > 0.8):\")\n",
    "if high_corr_pairs:\n",
    "    for pair in high_corr_pairs[:10]:  # Show top 10\n",
    "        print(f\"  {pair['Feature1']} <-> {pair['Feature2']}: {pair['Correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"  No highly correlated pairs found.\")\n",
    "\n",
    "# Display top target correlations\n",
    "print(f\"\\nTop 10 Features Correlated with Target:\")\n",
    "for feature, corr in target_correlations.head(10).items():\n",
    "    print(f\"  {feature}: {corr:.4f}\")\n",
    "\n",
    "# Create correlation heatmap for selected features\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Select top correlated features with target for visualization\n",
    "top_features = target_correlations.head(20).index.tolist()\n",
    "selected_data = feature_matrix[top_features + ['target']]\n",
    "selected_corr = selected_data.corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(selected_corr))\n",
    "sns.heatmap(selected_corr, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('Feature Correlation Heatmap (Top 20 Features)', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature-Target correlation bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_15_corrs = target_correlations.head(15)\n",
    "colors = ['red' if x < 0 else 'blue' for x in feature_matrix[top_15_corrs.index].corrwith(feature_matrix['target'])]\n",
    "\n",
    "bars = plt.barh(range(len(top_15_corrs)), top_15_corrs.values, color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_15_corrs)), top_15_corrs.index)\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.title('Top 15 Features by Target Correlation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, top_15_corrs.values)):\n",
    "    plt.text(val + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612072ac",
   "metadata": {},
   "source": [
    "## Section 5: Principal Component Analysis (PCA)\n",
    "\n",
    "### Dimensionality Reduction Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Feature Matrix] --> B[Standardization]\n",
    "    B --> C[PCA Fitting]\n",
    "    C --> D[Component Selection]\n",
    "    D --> E[Transformation]\n",
    "    E --> F[Reduced Dataset]\n",
    "    \n",
    "    C --> C1[Explained Variance]\n",
    "    D --> D1[80-95% Threshold]\n",
    "```\n",
    "\n",
    "PCA helps us reduce dimensionality while preserving the most important variance in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "def perform_pca_analysis(data, target_col='target', n_components=None, variance_threshold=0.90):\n",
    "    \"\"\"Perform PCA analysis with comprehensive reporting.\"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine number of components if not specified\n",
    "    if n_components is None:\n",
    "        # Find components needed for variance threshold\n",
    "        pca_temp = PCA()\n",
    "        pca_temp.fit(X_scaled)\n",
    "        cumsum_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= variance_threshold) + 1\n",
    "        print(f\"Selected {n_components} components to explain {variance_threshold*100}% of variance\")\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create DataFrame with principal components\n",
    "    pca_df = pd.DataFrame(\n",
    "        X_pca, \n",
    "        index=data.index,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    pca_df[target_col] = y\n",
    "    \n",
    "    return pca, pca_df, scaler, X_scaled\n",
    "\n",
    "# Perform PCA analysis\n",
    "print(\"Performing Principal Component Analysis...\")\n",
    "pca_model, pca_data, feature_scaler, scaled_features = perform_pca_analysis(feature_matrix, n_components=10)\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"Number of components: {pca_model.n_components_}\")\n",
    "print(f\"Total variance explained: {pca_model.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Display explained variance by component\n",
    "variance_explained = pca_model.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "\n",
    "print(f\"\\nVariance Explained by Component:\")\n",
    "for i, (var, cum_var) in enumerate(zip(variance_explained, cumulative_variance)):\n",
    "    print(f\"  PC{i+1}: {var:.4f} (Cumulative: {cum_var:.4f})\")\n",
    "\n",
    "# Visualize PCA results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Explained variance by component\n",
    "axes[0, 0].bar(range(1, len(variance_explained) + 1), variance_explained, alpha=0.7, color='blue')\n",
    "axes[0, 0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-', color='red')\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0, 0].set_title('PCA Explained Variance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend(['Individual', 'Cumulative'])\n",
    "\n",
    "# 2. First two principal components\n",
    "scatter = axes[0, 1].scatter(pca_data['PC1'], pca_data['PC2'], c=pca_data['target'], \n",
    "                           cmap='RdBu_r', alpha=0.6, s=20)\n",
    "axes[0, 1].set_xlabel('First Principal Component')\n",
    "axes[0, 1].set_ylabel('Second Principal Component')\n",
    "axes[0, 1].set_title('Data in PC Space (colored by target)')\n",
    "plt.colorbar(scatter, ax=axes[0, 1])\n",
    "\n",
    "# 3. Feature loadings for PC1\n",
    "feature_names = feature_matrix.drop(columns=['target']).columns\n",
    "pc1_loadings = pca_model.components_[0]\n",
    "top_features_pc1 = np.argsort(np.abs(pc1_loadings))[-10:]\n",
    "axes[1, 0].barh(range(10), pc1_loadings[top_features_pc1])\n",
    "axes[1, 0].set_yticks(range(10))\n",
    "axes[1, 0].set_yticklabels([feature_names[i] for i in top_features_pc1])\n",
    "axes[1, 0].set_xlabel('Loading')\n",
    "axes[1, 0].set_title('Top 10 Feature Loadings for PC1')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature loadings for PC2\n",
    "pc2_loadings = pca_model.components_[1]\n",
    "top_features_pc2 = np.argsort(np.abs(pc2_loadings))[-10:]\n",
    "axes[1, 1].barh(range(10), pc2_loadings[top_features_pc2])\n",
    "axes[1, 1].set_yticks(range(10))\n",
    "axes[1, 1].set_yticklabels([feature_names[i] for i in top_features_pc2])\n",
    "axes[1, 1].set_xlabel('Loading')\n",
    "axes[1, 1].set_title('Top 10 Feature Loadings for PC2')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze principal components\n",
    "print(f\"\\nPrincipal Component Analysis:\")\n",
    "print(f\"PC1 (explains {variance_explained[0]:.1%} of variance):\")\n",
    "pc1_top_features = np.argsort(np.abs(pc1_loadings))[-5:]\n",
    "for idx in reversed(pc1_top_features):\n",
    "    print(f\"  {feature_names[idx]}: {pc1_loadings[idx]:.3f}\")\n",
    "\n",
    "print(f\"\\nPC2 (explains {variance_explained[1]:.1%} of variance):\")\n",
    "pc2_top_features = np.argsort(np.abs(pc2_loadings))[-5:]\n",
    "for idx in reversed(pc2_top_features):\n",
    "    print(f\"  {feature_names[idx]}: {pc2_loadings[idx]:.3f}\")\n",
    "\n",
    "print(f\"\\nPCA transformation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b442647",
   "metadata": {},
   "source": [
    "## Section 6: Feature Importance Analysis\n",
    "\n",
    "Using tree-based methods to identify the most predictive features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df05e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis using Random Forest\n",
    "def analyze_feature_importance(data, target_col='target', n_estimators=100):\n",
    "    \"\"\"Calculate feature importance using Random Forest.\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    print(f\"Training Random Forest on {X_clean.shape[0]} samples with {X_clean.shape[1]} features...\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=SEED,\n",
    "        n_jobs=1 if SAMPLE_MODE else -1,\n",
    "        max_depth=10 if SAMPLE_MODE else None\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_clean, y_clean)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importance_scores = rf.feature_importances_\n",
    "    feature_names = X_clean.columns\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return rf, importance_df, X_clean, y_clean\n",
    "\n",
    "# Perform feature importance analysis\n",
    "print(\"Analyzing feature importance with Random Forest...\")\n",
    "rf_model, importance_rankings, X_clean, y_clean = analyze_feature_importance(feature_matrix)\n",
    "\n",
    "print(f\"\\nRandom Forest Model Performance:\")\n",
    "train_score = rf_model.score(X_clean, y_clean)\n",
    "print(f\"Training R²: {train_score:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 15 Most Important Features:\")\n",
    "for idx, row in importance_rankings.head(15).iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Top 20 feature importance bar plot\n",
    "top_20_features = importance_rankings.head(20)\n",
    "axes[0, 0].barh(range(len(top_20_features)), top_20_features['Importance'][::-1], alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(top_20_features)))\n",
    "axes[0, 0].set_yticklabels(top_20_features['Feature'][::-1])\n",
    "axes[0, 0].set_xlabel('Importance Score')\n",
    "axes[0, 0].set_title('Top 20 Features by Random Forest Importance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature importance distribution\n",
    "axes[0, 1].hist(importance_rankings['Importance'], bins=30, alpha=0.7, color='green')\n",
    "axes[0, 1].set_xlabel('Importance Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Feature Importance Scores')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cumulative importance\n",
    "cumulative_importance = np.cumsum(importance_rankings['Importance'])\n",
    "axes[1, 0].plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-')\n",
    "axes[1, 0].axhline(y=0.8, color='r', linestyle='--', label='80% Threshold')\n",
    "axes[1, 0].axhline(y=0.9, color='orange', linestyle='--', label='90% Threshold')\n",
    "axes[1, 0].set_xlabel('Number of Features')\n",
    "axes[1, 0].set_ylabel('Cumulative Importance')\n",
    "axes[1, 0].set_title('Cumulative Feature Importance')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature category importance\n",
    "categories = {\n",
    "    'Returns': [f for f in importance_rankings['Feature'] if 'return' in f and 'lag' not in f],\n",
    "    'Moving Averages': [f for f in importance_rankings['Feature'] if any(x in f for x in ['SMA', 'EMA', 'price_to'])],\n",
    "    'Volatility': [f for f in importance_rankings['Feature'] if 'volatility' in f or 'range' in f],\n",
    "    'Technical': [f for f in importance_rankings['Feature'] if any(x in f for x in ['RSI', 'MACD', 'BB', 'momentum', 'ROC'])],\n",
    "    'Lag Features': [f for f in importance_rankings['Feature'] if 'lag' in f]\n",
    "}\n",
    "\n",
    "category_importance = {}\n",
    "for category, features in categories.items():\n",
    "    category_features = importance_rankings[importance_rankings['Feature'].isin(features)]\n",
    "    category_importance[category] = category_features['Importance'].sum()\n",
    "\n",
    "category_names = list(category_importance.keys())\n",
    "category_scores = list(category_importance.values())\n",
    "\n",
    "axes[1, 1].pie(category_scores, labels=category_names, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Feature Importance by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find features needed for 80% and 90% importance\n",
    "cumulative_importance = np.cumsum(importance_rankings['Importance'])\n",
    "features_80 = np.argmax(cumulative_importance >= 0.8) + 1\n",
    "features_90 = np.argmax(cumulative_importance >= 0.9) + 1\n",
    "\n",
    "print(f\"\\nFeature Selection Analysis:\")\n",
    "print(f\"Features needed for 80% importance: {features_80} ({features_80/len(importance_rankings)*100:.1f}%)\")\n",
    "print(f\"Features needed for 90% importance: {features_90} ({features_90/len(importance_rankings)*100:.1f}%)\")\n",
    "\n",
    "# Export top features for later use\n",
    "top_features_80 = importance_rankings.head(features_80)['Feature'].tolist()\n",
    "top_features_90 = importance_rankings.head(features_90)['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nFeature importance analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Engineered Dataset\n",
    "def export_engineered_data(feature_data, importance_data, filename='engineered_financial_data.csv'):\n",
    "    \"\"\"Export the engineered dataset with metadata.\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = '/kaggle/working' if '/kaggle' in os.getcwd() else 'output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Full dataset export\n",
    "    full_path = os.path.join(output_dir, filename)\n",
    "    feature_data.to_csv(full_path)\n",
    "    \n",
    "    # Top features dataset (80% importance)\n",
    "    top_features = importance_data.head(features_80)['Feature'].tolist() + ['target']\n",
    "    top_features_data = feature_data[top_features]\n",
    "    top_features_path = os.path.join(output_dir, 'top_features_' + filename)\n",
    "    top_features_data.to_csv(top_features_path)\n",
    "    \n",
    "    # Feature metadata\n",
    "    metadata = {\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'data_source': data_source,\n",
    "        'sample_mode': SAMPLE_MODE,\n",
    "        'total_features': len(feature_data.columns) - 1,\n",
    "        'total_samples': len(feature_data),\n",
    "        'date_range': f\"{feature_data.index.min().date()} to {feature_data.index.max().date()}\",\n",
    "        'top_features_80_pct': features_80,\n",
    "        'top_features_90_pct': features_90\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, 'feature_metadata.json')\n",
    "    import json\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Exported datasets:\")\n",
    "    print(f\"  Full dataset: {full_path} ({feature_data.shape})\")\n",
    "    print(f\"  Top features: {top_features_path} ({top_features_data.shape})\")\n",
    "    print(f\"  Metadata: {metadata_path}\")\n",
    "    \n",
    "    return full_path, top_features_path, metadata_path\n",
    "\n",
    "# Export the engineered datasets\n",
    "export_paths = export_engineered_data(feature_matrix, importance_rankings)\n",
    "\n",
    "print(f\"\\n🎉 Feature Engineering Pipeline Complete!\")\n",
    "print(f\"📊 Created {len(feature_matrix.columns)-1} features from raw financial data\")\n",
    "print(f\"📈 Random Forest R²: {train_score:.4f}\")\n",
    "print(f\"🔍 Top feature: {importance_rankings.iloc[0]['Feature']} (importance: {importance_rankings.iloc[0]['Importance']:.4f})\")\n",
    "print(f\"💾 Datasets exported and ready for modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6e0db",
   "metadata": {},
   "source": [
    "## Section 7: Student Exercises\n",
    "\n",
    "Complete the following exercises to master advanced feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569be205",
   "metadata": {},
   "source": [
    "### Exercise 1: Manual MACD Calculation\n",
    "\n",
    "Implement MACD (Moving Average Convergence Divergence) from scratch and compare with any existing implementation.\n",
    "\n",
    "**Components:**\n",
    "- MACD Line = EMA(12) - EMA(26)\n",
    "- Signal Line = EMA(9) of MACD Line  \n",
    "- Histogram = MACD Line - Signal Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e25119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1 - Manual MACD Calculation\n",
    "# Your task:\n",
    "# 1. Implement MACD calculation manually using pandas ewm() function\n",
    "# 2. Create MACD line, signal line, and histogram\n",
    "# 3. Visualize the MACD components with the price chart\n",
    "# 4. Identify bullish/bearish crossovers (MACD > Signal vs MACD < Signal)\n",
    "\n",
    "def calculate_macd_manual(prices, fast_period=12, slow_period=26, signal_period=9):\n",
    "    \"\"\"\n",
    "    TODO: Implement MACD calculation manually\n",
    "    \n",
    "    Parameters:\n",
    "    - prices: pandas Series of price data\n",
    "    - fast_period: Period for fast EMA (default 12)\n",
    "    - slow_period: Period for slow EMA (default 26)  \n",
    "    - signal_period: Period for signal line EMA (default 9)\n",
    "    \n",
    "    Returns:\n",
    "    - macd_line: Fast EMA - Slow EMA\n",
    "    - signal_line: EMA of MACD line\n",
    "    - histogram: MACD line - Signal line\n",
    "    \"\"\"\n",
    "    # TODO: Calculate fast and slow EMAs\n",
    "    # fast_ema = ???\n",
    "    # slow_ema = ???\n",
    "    \n",
    "    # TODO: Calculate MACD line\n",
    "    # macd_line = ???\n",
    "    \n",
    "    # TODO: Calculate signal line\n",
    "    # signal_line = ???\n",
    "    \n",
    "    # TODO: Calculate histogram\n",
    "    # histogram = ???\n",
    "    \n",
    "    # return macd_line, signal_line, histogram\n",
    "    pass\n",
    "\n",
    "# TODO: Test your MACD implementation\n",
    "# macd, signal, hist = calculate_macd_manual(cleaned_data['Close'])\n",
    "\n",
    "# TODO: Create visualization comparing price and MACD\n",
    "# Include subplots showing:\n",
    "# 1. Price chart\n",
    "# 2. MACD line and signal line\n",
    "# 3. MACD histogram\n",
    "\n",
    "print(\"Exercise 1: Implement the MACD calculation above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32ede3",
   "metadata": {},
   "source": [
    "### Exercise 2: Bollinger Bands Implementation\n",
    "\n",
    "Create Bollinger Bands and analyze price position relative to the bands.\n",
    "\n",
    "**Components:**\n",
    "- Middle Band = 20-period Simple Moving Average\n",
    "- Upper Band = Middle Band + (2 × Standard Deviation)\n",
    "- Lower Band = Middle Band - (2 × Standard Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37785a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Bollinger Bands Implementation\n",
    "# Your task:\n",
    "# 1. Calculate 20-period SMA as middle band\n",
    "# 2. Calculate rolling standard deviation\n",
    "# 3. Create upper and lower bands (middle ± 2×std)\n",
    "# 4. Calculate band width and price position within bands\n",
    "# 5. Identify when price touches or breaks bands\n",
    "\n",
    "def calculate_bollinger_bands(prices, window=20, num_std=2):\n",
    "    \"\"\"\n",
    "    TODO: Implement Bollinger Bands calculation\n",
    "    \n",
    "    Parameters:\n",
    "    - prices: pandas Series of price data\n",
    "    - window: Period for moving average and std (default 20)\n",
    "    - num_std: Number of standard deviations (default 2)\n",
    "    \n",
    "    Returns:\n",
    "    - upper_band: Upper Bollinger Band\n",
    "    - middle_band: Middle Bollinger Band (SMA)\n",
    "    - lower_band: Lower Bollinger Band\n",
    "    - band_width: (Upper - Lower) / Middle\n",
    "    - price_position: (Price - Lower) / (Upper - Lower)\n",
    "    \"\"\"\n",
    "    # TODO: Calculate middle band (SMA)\n",
    "    middle_band = prices.rolling(window=window).mean()\n",
    "    \n",
    "    # TODO: Calculate rolling standard deviation\n",
    "    rolling_std = prices.rolling(window=window).std()\n",
    "    \n",
    "    # TODO: Calculate upper and lower bands\n",
    "    upper_band = middle_band + (num_std * rolling_std)\n",
    "    lower_band = middle_band - (num_std * rolling_std)\n",
    "    \n",
    "    # TODO: Calculate band width (volatility measure)\n",
    "    band_width = (upper_band - lower_band) / middle_band\n",
    "    \n",
    "    # TODO: Calculate price position within bands (0 to 1)\n",
    "    price_position = (prices - lower_band) / (upper_band - lower_band)\n",
    "    \n",
    "    return upper_band, middle_band, lower_band, band_width, price_position\n",
    "\n",
    "# Test your Bollinger Bands implementation\n",
    "upper, middle, lower, width, position = calculate_bollinger_bands(df['close'])\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 16))\n",
    "\n",
    "# 1. Price chart with Bollinger Bands\n",
    "axes[0].plot(df.index, df['close'], label='Close Price', color='black', linewidth=2)\n",
    "axes[0].plot(df.index, upper, label='Upper Band', color='red', alpha=0.7)\n",
    "axes[0].plot(df.index, middle, label='Middle Band (SMA)', color='blue', alpha=0.7)\n",
    "axes[0].plot(df.index, lower, label='Lower Band', color='green', alpha=0.7)\n",
    "axes[0].fill_between(df.index, upper, lower, alpha=0.1, color='gray')\n",
    "axes[0].set_title('Bollinger Bands Analysis')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Band width over time (volatility indicator)\n",
    "axes[1].plot(df.index, width, color='purple', linewidth=2)\n",
    "axes[1].set_title('Bollinger Band Width (Volatility Measure)')\n",
    "axes[1].set_ylabel('Band Width')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=width.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {width.mean():.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Price position within bands\n",
    "axes[2].plot(df.index, position, color='orange', linewidth=2)\n",
    "axes[2].axhline(y=0.5, color='blue', linestyle='-', alpha=0.5, label='Middle')\n",
    "axes[2].axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Upper Threshold (80%)')\n",
    "axes[2].axhline(y=0.2, color='green', linestyle='--', alpha=0.7, label='Lower Threshold (20%)')\n",
    "axes[2].set_title('Price Position Within Bollinger Bands (0=Lower Band, 1=Upper Band)')\n",
    "axes[2].set_ylabel('Position')\n",
    "axes[2].set_ylim(-0.1, 1.1)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Price position histogram\n",
    "axes[3].hist(position.dropna(), bins=50, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[3].axvline(x=0.5, color='blue', linestyle='-', alpha=0.7, label='Middle (50%)')\n",
    "axes[3].axvline(x=0.8, color='red', linestyle='--', alpha=0.7, label='Upper Threshold (80%)')\n",
    "axes[3].axvline(x=0.2, color='green', linestyle='--', alpha=0.7, label='Lower Threshold (20%)')\n",
    "axes[3].set_title('Distribution of Price Position Within Bands')\n",
    "axes[3].set_xlabel('Price Position')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis: Band Breakouts and Trading Signals\n",
    "breakouts_upper = position > 1.0  # Price above upper band\n",
    "breakouts_lower = position < 0.0  # Price below lower band\n",
    "squeeze_periods = width < width.quantile(0.2)  # Low volatility periods\n",
    "\n",
    "print(\"Bollinger Bands Analysis:\")\n",
    "print(f\"Upper band breakouts: {breakouts_upper.sum()} occurrences\")\n",
    "print(f\"Lower band breakouts: {breakouts_lower.sum()} occurrences\")\n",
    "print(f\"Squeeze periods (low volatility): {squeeze_periods.sum()} days\")\n",
    "print(f\"Average band width: {width.mean():.4f}\")\n",
    "print(f\"Price position statistics:\")\n",
    "print(position.describe())\n",
    "\n",
    "# Analyze the relationship between band width and future volatility\n",
    "future_vol = df['close'].pct_change().rolling(5).std().shift(-5)  # 5-day forward volatility\n",
    "width_vol_corr = width.corr(future_vol)\n",
    "print(f\"\\nCorrelation between band width and 5-day forward volatility: {width_vol_corr:.4f}\")\n",
    "\n",
    "print(\"\\nExercise 2 completed! Bollinger Bands implemented with comprehensive analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec8a1f",
   "metadata": {},
   "source": [
    "### Exercise 3: PCA vs Feature Importance Comparison\n",
    "\n",
    "Compare the top components from PCA with the top features from Random Forest importance ranking.\n",
    "\n",
    "**Analysis Points:**\n",
    "- Which features load heavily on the first few principal components?\n",
    "- How do PCA loadings correlate with feature importance rankings?\n",
    "- When might PCA be preferred over feature selection and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: PCA vs Feature Importance Comparison\n",
    "# Compare PCA loadings with Random Forest feature importance rankings\n",
    "\n",
    "# Prepare feature matrix for comparison (using our engineered features)\n",
    "feature_columns = ['returns', 'log_returns', 'sma_5', 'sma_20', 'volatility_5', \n",
    "                   'volatility_20', 'rsi', 'macd', 'macd_signal', 'bb_upper', \n",
    "                   'bb_middle', 'bb_lower', 'bb_width', 'bb_position']\n",
    "\n",
    "# Create feature matrix (drop NaN values)\n",
    "features_df = df[feature_columns].dropna()\n",
    "print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "print(f\"Features used: {list(features_df.columns)}\")\n",
    "\n",
    "# Standardize features for PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_df)\n",
    "\n",
    "# 1. PCA Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PCA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA()\n",
    "pca_components = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"Explained variance by component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_[:6]):\n",
    "    print(f\"PC{i+1}: {var:.3f} ({cumsum_var[i]:.3f} cumulative)\")\n",
    "\n",
    "# Get loadings (components) for first 3 PCs\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_[:3].T,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=features_df.columns\n",
    ")\n",
    "\n",
    "print(f\"\\nPCA Loadings for first 3 components:\")\n",
    "print(loadings.round(3))\n",
    "\n",
    "# Find features with highest absolute loadings for each PC\n",
    "print(f\"\\nTop contributing features per component:\")\n",
    "for pc in ['PC1', 'PC2', 'PC3']:\n",
    "    top_features = loadings[pc].abs().sort_values(ascending=False).head(3)\n",
    "    print(f\"{pc}: {', '.join([f'{feat}({val:.3f})' for feat, val in top_features.items()])}\")\n",
    "\n",
    "# 2. Random Forest Feature Importance\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST FEATURE IMPORTANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create target variable (next day returns for prediction)\n",
    "target = df['returns'].shift(-1).dropna()\n",
    "features_for_rf = features_df.iloc[:-1]  # Remove last row to match target\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf.fit(features_for_rf, target)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features_for_rf.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Random Forest Feature Importance Rankings:\")\n",
    "for i, row in importance_df.iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# 3. Comparison Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PCA vs FEATURE IMPORTANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: PCA Explained Variance\n",
    "axes[0, 0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "               pca.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].plot(range(1, len(cumsum_var) + 1), cumsum_var, \n",
    "                'ro-', markersize=6, color='red')\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0, 0].set_title('PCA Explained Variance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% Threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Feature Importance\n",
    "top_features = importance_df.head(10)\n",
    "axes[0, 1].barh(range(len(top_features)), top_features['importance'], \n",
    "                color='forestgreen', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(top_features)))\n",
    "axes[0, 1].set_yticklabels(top_features['feature'])\n",
    "axes[0, 1].set_xlabel('Importance Score')\n",
    "axes[0, 1].set_title('Random Forest Feature Importance (Top 10)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PCA Loadings Heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(loadings.T, annot=True, cmap='RdBu_r', center=0, \n",
    "            ax=axes[1, 0], cbar_kws={'label': 'Loading'})\n",
    "axes[1, 0].set_title('PCA Loadings (First 3 Components)')\n",
    "axes[1, 0].set_xlabel('Features')\n",
    "\n",
    "# Plot 4: Correlation between PCA loadings and RF importance\n",
    "# Calculate correlation for PC1 loadings vs RF importance\n",
    "pc1_loadings_abs = loadings['PC1'].abs()\n",
    "rf_importance_dict = dict(zip(importance_df['feature'], importance_df['importance']))\n",
    "rf_scores_aligned = pd.Series([rf_importance_dict[feat] for feat in pc1_loadings_abs.index])\n",
    "\n",
    "axes[1, 1].scatter(pc1_loadings_abs, rf_scores_aligned, alpha=0.7, s=60, color='purple')\n",
    "for i, feature in enumerate(pc1_loadings_abs.index):\n",
    "    axes[1, 1].annotate(feature, (pc1_loadings_abs.iloc[i], rf_scores_aligned.iloc[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "correlation = pc1_loadings_abs.corr(rf_scores_aligned)\n",
    "axes[1, 1].set_xlabel('|PC1 Loading|')\n",
    "axes[1, 1].set_ylabel('RF Feature Importance')\n",
    "axes[1, 1].set_title(f'PC1 Loadings vs RF Importance\\n(Correlation: {correlation:.3f})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Analysis Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find components needed for 95% variance\n",
    "components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "print(f\"Components needed for 95% variance: {components_95}\")\n",
    "\n",
    "# Top features from each method\n",
    "top_rf_features = set(importance_df.head(5)['feature'])\n",
    "top_pc1_features = set(loadings['PC1'].abs().sort_values(ascending=False).head(5).index)\n",
    "\n",
    "common_features = top_rf_features.intersection(top_pc1_features)\n",
    "print(f\"Top 5 RF features: {top_rf_features}\")\n",
    "print(f\"Top 5 PC1 features: {top_pc1_features}\")\n",
    "print(f\"Common top features: {common_features}\")\n",
    "\n",
    "# When to use each method\n",
    "print(f\"\\nRECOMMENDATIONS:\")\n",
    "print(f\"• Use PCA when:\")\n",
    "print(f\"  - High multicollinearity (VIF > 10)\")\n",
    "print(f\"  - Need dimensionality reduction ({components_95} components vs {len(feature_columns)} features)\")\n",
    "print(f\"  - Want orthogonal features\")\n",
    "print(f\"  - Interpretability is less important\")\n",
    "print(f\"• Use Feature Selection when:\")\n",
    "print(f\"  - Interpretability is crucial\")\n",
    "print(f\"  - Want to maintain original feature meanings\")\n",
    "print(f\"  - Low correlation between features\")\n",
    "print(f\"  - Need domain-specific feature insights\")\n",
    "\n",
    "print(f\"\\nExercise 3 completed! PCA vs Feature Importance analysis finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
