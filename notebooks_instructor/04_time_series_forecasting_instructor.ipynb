{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d876718e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Stock Forecasting using ARIMA, Prophet & LSTM (INSTRUCTOR VERSION)\"\n",
    "week: 4\n",
    "author: \"Praveen Kumar\"\n",
    "date: 2025-10-07\n",
    "version: v1.0\n",
    "instructor_only: true\n",
    "---\n",
    "\n",
    "# Week 4: Stock Forecasting using ARIMA, Prophet & LSTM\n",
    "## **INSTRUCTOR VERSION** üéì\n",
    "\n",
    "This notebook contains **complete solutions** for all exercises and additional teaching notes.\n",
    "\n",
    "‚ö†Ô∏è **CONFIDENTIAL**: Do not share this version with students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a02826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SEED = 42\n",
    "SAMPLE_MODE = True  # Use subset for faster execution\n",
    "DATA_PATH = \"data/synthetic/stock_prices.csv\"\n",
    "SYMBOL = \"AAPL\"  # Stock symbol to forecast\n",
    "\n",
    "# INSTRUCTOR ONLY: Additional parameters for advanced models\n",
    "ADVANCED_MODELS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY: Setup with additional imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series specific\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Prophet\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Prophet not available. Installing...\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available for LSTM modeling\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(SEED)\n",
    "if TF_AVAILABLE:\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"üéì INSTRUCTOR VERSION - Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca5872",
   "metadata": {},
   "source": [
    "## INSTRUCTOR NOTE: Data Loading Strategy\n",
    "\n",
    "**Teaching Point**: Always provide robust data loading with multiple fallback options for classroom environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY: Enhanced data loading with multiple fallbacks\n",
    "def load_stock_data_instructor(symbol=\"AAPL\", period=\"5y\", sample_mode=True):\n",
    "    \"\"\"Enhanced data loading with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Loading {symbol} data...\")\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"No data returned from yfinance\")\n",
    "            \n",
    "        # Convert to lowercase columns for consistency\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # INSTRUCTOR: Quality checks\n",
    "        print(f\"‚úÖ Data quality check:\")\n",
    "        print(f\"   - Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        print(f\"   - Missing values: {df.isnull().sum().sum()}\")\n",
    "        print(f\"   - Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "        \n",
    "        # Subset for sample mode\n",
    "        if sample_mode and len(df) > 1000:\n",
    "            df = df.tail(1000)\n",
    "            print(f\"üìä Using last 1000 days for sample mode\")\n",
    "            \n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} days of {symbol} data\")\n",
    "        return df, 'real'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {symbol}: {e}\")\n",
    "        print(\"üîÑ Falling back to synthetic data...\")\n",
    "        return generate_synthetic_stock_data_instructor(sample_mode), 'synthetic'\n",
    "\n",
    "def generate_synthetic_stock_data_instructor(sample_mode=True):\n",
    "    \"\"\"INSTRUCTOR: Generate realistic synthetic financial time series\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    n_days = 1000 if sample_mode else 2000\n",
    "    dates = pd.date_range(start='2020-01-01', periods=n_days, freq='D')\n",
    "    \n",
    "    # INSTRUCTOR: More sophisticated synthetic data generation\n",
    "    # 1. Generate returns with volatility clustering (GARCH-like)\n",
    "    returns = np.zeros(n_days)\n",
    "    volatility = np.zeros(n_days)\n",
    "    volatility[0] = 0.02  # Initial volatility\n",
    "    \n",
    "    for i in range(1, n_days):\n",
    "        # GARCH(1,1) type volatility\n",
    "        volatility[i] = 0.00001 + 0.05 * returns[i-1]**2 + 0.94 * volatility[i-1]\n",
    "        returns[i] = np.random.normal(0.0005, np.sqrt(volatility[i]))\n",
    "    \n",
    "    # 2. Convert to prices\n",
    "    prices = 100 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # 3. Add weekly seasonality (higher returns on Fridays)\n",
    "    weekday_effect = np.sin(2 * np.pi * np.arange(n_days) / 7) * 0.001\n",
    "    prices *= np.exp(np.cumsum(weekday_effect))\n",
    "    \n",
    "    # 4. Generate OHLC data\n",
    "    daily_vol = np.random.uniform(0.005, 0.025, n_days)\n",
    "    high_prices = prices * (1 + daily_vol)\n",
    "    low_prices = prices * (1 - daily_vol)\n",
    "    open_prices = np.roll(prices, 1)\n",
    "    open_prices[0] = prices[0]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'close': prices,\n",
    "        'open': open_prices,\n",
    "        'high': high_prices,\n",
    "        'low': low_prices,\n",
    "        'volume': np.random.lognormal(15, 0.5, n_days).astype(int)\n",
    "    }, index=dates)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(df)} days of sophisticated synthetic data\")\n",
    "    print(f\"   - Includes volatility clustering and weekly seasonality\")\n",
    "    return df\n",
    "\n",
    "# Load data with enhanced function\n",
    "df, data_source = load_stock_data_instructor(SYMBOL, period=\"5y\", sample_mode=SAMPLE_MODE)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"Source: {data_source}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8682b3",
   "metadata": {},
   "source": [
    "## INSTRUCTOR NOTE: Advanced EDA\n",
    "\n",
    "**Teaching Points:**\n",
    "- Always decompose time series into trend, seasonal, and residual components\n",
    "- Check for volatility clustering in financial data\n",
    "- Demonstrate autocorrelation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY: Advanced Exploratory Data Analysis\n",
    "def advanced_eda_instructor(df):\n",
    "    \"\"\"Comprehensive EDA for financial time series\"\"\"\n",
    "    \n",
    "    # Calculate returns\n",
    "    returns = df['close'].pct_change().dropna()\n",
    "    log_returns = np.log(df['close'] / df['close'].shift(1)).dropna()\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    \n",
    "    # 1. Price and volume\n",
    "    axes[0, 0].plot(df.index, df['close'], linewidth=1.5, color='blue')\n",
    "    axes[0, 0].set_title('Stock Price Over Time')\n",
    "    axes[0, 0].set_ylabel('Price ($)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Volume\n",
    "    axes[0, 1].bar(df.index, df['volume'], width=1, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Trading Volume')\n",
    "    axes[0, 1].set_ylabel('Volume')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Returns distribution\n",
    "    axes[0, 2].hist(returns, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[0, 2].axvline(returns.mean(), color='blue', linestyle='--', \n",
    "                      label=f'Mean: {returns.mean():.4f}')\n",
    "    axes[0, 2].set_title('Returns Distribution')\n",
    "    axes[0, 2].set_xlabel('Daily Returns')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Autocorrelation of returns\n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    plot_acf(returns.dropna(), lags=40, ax=axes[1, 0], alpha=0.05)\n",
    "    axes[1, 0].set_title('Returns Autocorrelation')\n",
    "    \n",
    "    # 5. Autocorrelation of squared returns (volatility clustering)\n",
    "    plot_acf(returns.dropna()**2, lags=40, ax=axes[1, 1], alpha=0.05)\n",
    "    axes[1, 1].set_title('Squared Returns ACF (Volatility Clustering)')\n",
    "    \n",
    "    # 6. Rolling volatility\n",
    "    rolling_vol = returns.rolling(window=30).std() * np.sqrt(252)  # Annualized\n",
    "    axes[1, 2].plot(rolling_vol.index, rolling_vol, color='purple', linewidth=1.5)\n",
    "    axes[1, 2].set_title('30-day Rolling Volatility (Annualized)')\n",
    "    axes[1, 2].set_ylabel('Volatility')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Q-Q plot for normality check\n",
    "    from scipy import stats\n",
    "    stats.probplot(returns.dropna(), dist=\"norm\", plot=axes[2, 0])\n",
    "    axes[2, 0].set_title('Q-Q Plot (Normality Check)')\n",
    "    \n",
    "    # 8. Seasonal decomposition (if enough data)\n",
    "    if len(df) > 365:\n",
    "        # Resample to weekly for seasonal decomposition\n",
    "        weekly_prices = df['close'].resample('W').last()\n",
    "        if len(weekly_prices) > 104:  # At least 2 years of weekly data\n",
    "            decomposition = seasonal_decompose(weekly_prices, model='multiplicative', period=52)\n",
    "            axes[2, 1].plot(decomposition.trend.dropna())\n",
    "            axes[2, 1].set_title('Trend Component (Weekly)')\n",
    "            axes[2, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[2, 2].plot(decomposition.seasonal[:52])  # First year\n",
    "            axes[2, 2].set_title('Seasonal Component (First Year)')\n",
    "            axes[2, 2].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[2, 1].text(0.5, 0.5, 'Insufficient data\\nfor decomposition', \n",
    "                           ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "            axes[2, 2].text(0.5, 0.5, 'Insufficient data\\nfor decomposition', \n",
    "                           ha='center', va='center', transform=axes[2, 2].transAxes)\n",
    "    else:\n",
    "        axes[2, 1].text(0.5, 0.5, 'Insufficient data\\nfor decomposition', \n",
    "                       ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "        axes[2, 2].text(0.5, 0.5, 'Insufficient data\\nfor decomposition', \n",
    "                       ha='center', va='center', transform=axes[2, 2].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical summary\n",
    "    print(\"üìä STATISTICAL SUMMARY:\")\n",
    "    print(f\"Returns - Mean: {returns.mean():.6f}, Std: {returns.std():.6f}\")\n",
    "    print(f\"Annualized Volatility: {returns.std() * np.sqrt(252):.4f}\")\n",
    "    print(f\"Skewness: {returns.skew():.4f}\")\n",
    "    print(f\"Kurtosis: {returns.kurtosis():.4f}\")\n",
    "    \n",
    "    # Test for normality\n",
    "    from scipy.stats import jarque_bera\n",
    "    jb_stat, jb_pvalue = jarque_bera(returns.dropna())\n",
    "    print(f\"Jarque-Bera test p-value: {jb_pvalue:.6f}\")\n",
    "    if jb_pvalue < 0.05:\n",
    "        print(\"‚ùå Returns are NOT normally distributed\")\n",
    "    else:\n",
    "        print(\"‚úÖ Returns appear normally distributed\")\n",
    "    \n",
    "    return returns, log_returns\n",
    "\n",
    "# Perform advanced EDA\n",
    "returns_series, log_returns_series = advanced_eda_instructor(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2caf48",
   "metadata": {},
   "source": [
    "## INSTRUCTOR SOLUTION: Exercise 1 - Exponential Smoothing Baseline\n",
    "\n",
    "**Teaching Point**: Always include simple baselines to validate that complex models add value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5163cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY: Complete solution for Exercise 1\n",
    "def build_exponential_smoothing_instructor(train_data, seasonal_periods=None):\n",
    "    \"\"\"Build Holt-Winters exponential smoothing model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Determine if we have enough data for seasonality\n",
    "        if seasonal_periods and len(train_data) >= 2 * seasonal_periods:\n",
    "            model = ExponentialSmoothing(\n",
    "                train_data,\n",
    "                trend='add',\n",
    "                seasonal='add',\n",
    "                seasonal_periods=seasonal_periods\n",
    "            )\n",
    "            print(f\"Using Holt-Winters with seasonality (period={seasonal_periods})\")\n",
    "        else:\n",
    "            model = ExponentialSmoothing(train_data, trend='add')\n",
    "            print(\"Using Holt's method (trend only)\")\n",
    "        \n",
    "        fitted_model = model.fit(optimized=True)\n",
    "        return fitted_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with trend model: {e}\")\n",
    "        # Fall back to simple exponential smoothing\n",
    "        model = ExponentialSmoothing(train_data)\n",
    "        fitted_model = model.fit(optimized=True)\n",
    "        print(\"Using Simple Exponential Smoothing\")\n",
    "        return fitted_model\n",
    "\n",
    "# INSTRUCTOR: Implement the complete exercise solution\n",
    "print(\"üéì INSTRUCTOR SOLUTION: Exercise 1 - Exponential Smoothing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the same data preparation as main notebook\n",
    "target_series = returns_series\n",
    "split_point = int(len(target_series) * 0.8)\n",
    "train_data = target_series[:split_point].copy()\n",
    "test_data = target_series[split_point:].copy()\n",
    "\n",
    "# Build exponential smoothing model\n",
    "exp_smooth_model = build_exponential_smoothing_instructor(train_data, seasonal_periods=5)\n",
    "\n",
    "# Generate forecasts\n",
    "exp_smooth_forecast = exp_smooth_model.forecast(steps=len(test_data))\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics_instructor(actual, predicted, model_name):\n",
    "    \"\"\"Enhanced metrics calculation with additional measures\"\"\"\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    # Additional metrics for instructor version\n",
    "    directional_accuracy = np.mean(np.sign(actual[1:]) == np.sign(predicted[1:])) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse, \n",
    "        'MAPE': mape,\n",
    "        'Directional_Accuracy': directional_accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.6f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "exp_smooth_metrics = calculate_metrics_instructor(test_data, exp_smooth_forecast, \"Exponential Smoothing\")\n",
    "\n",
    "# Visualization comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_data.index, test_data.values, label='Actual', linewidth=2, color='black')\n",
    "plt.plot(test_data.index, exp_smooth_forecast, label='Exp. Smoothing', linewidth=2, color='orange')\n",
    "plt.title('Exponential Smoothing Forecast')\n",
    "plt.ylabel('Returns')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Exercise 1 Complete: Exponential Smoothing implemented as baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff1e34",
   "metadata": {},
   "source": [
    "## INSTRUCTOR SOLUTION: Exercise 2 - Walk-Forward Validation\n",
    "\n",
    "**Teaching Point**: Demonstrate realistic model evaluation using walk-forward validation to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY: Complete solution for Exercise 2  \n",
    "def walk_forward_validation_instructor(data, model_type='arima', window_size=200, step_size=20):\n",
    "    \"\"\"Implement walk-forward validation for time series models\"\"\"\n",
    "    \n",
    "    print(f\"üéì INSTRUCTOR SOLUTION: Exercise 2 - Walk-Forward Validation\")\n",
    "    print(f\"Model: {model_type.upper()}, Window: {window_size}, Step: {step_size}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_forecasts = []\n",
    "    all_actuals = []\n",
    "    all_dates = []\n",
    "    validation_metrics = []\n",
    "    \n",
    "    # Start from minimum required window size\n",
    "    start_idx = window_size\n",
    "    \n",
    "    while start_idx + step_size < len(data):\n",
    "        # Define training window\n",
    "        train_end_idx = start_idx\n",
    "        test_start_idx = start_idx\n",
    "        test_end_idx = min(start_idx + step_size, len(data))\n",
    "        \n",
    "        # Get training and test data\n",
    "        train_window = data.iloc[:train_end_idx]\n",
    "        test_window = data.iloc[test_start_idx:test_end_idx]\n",
    "        \n",
    "        try:\n",
    "            if model_type.lower() == 'arima':\n",
    "                # Fit ARIMA model\n",
    "                model = ARIMA(train_window, order=(1, 0, 1))  # Simple order for demo\n",
    "                fitted_model = model.fit()\n",
    "                forecast = fitted_model.forecast(steps=len(test_window))\n",
    "                \n",
    "            elif model_type.lower() == 'exp_smooth':\n",
    "                # Fit exponential smoothing\n",
    "                model = ExponentialSmoothing(train_window, trend='add')\n",
    "                fitted_model = model.fit(optimized=True)\n",
    "                forecast = fitted_model.forecast(steps=len(test_window))\n",
    "                \n",
    "            # Store results\n",
    "            all_forecasts.extend(forecast)\n",
    "            all_actuals.extend(test_window.values)\n",
    "            all_dates.extend(test_window.index)\n",
    "            \n",
    "            # Calculate window metrics\n",
    "            window_rmse = np.sqrt(mean_squared_error(test_window, forecast))\n",
    "            window_mae = mean_absolute_error(test_window, forecast)\n",
    "            \n",
    "            validation_metrics.append({\n",
    "                'window_end': train_window.index[-1],\n",
    "                'forecast_start': test_window.index[0],\n",
    "                'rmse': window_rmse,\n",
    "                'mae': window_mae,\n",
    "                'train_size': len(train_window),\n",
    "                'test_size': len(test_window)\n",
    "            })\n",
    "            \n",
    "            print(f\"Window {len(validation_metrics):2d}: Train to {train_window.index[-1].date()}, \"\n",
    "                  f\"Forecast {len(test_window)} days, RMSE: {window_rmse:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in window ending {train_window.index[-1].date()}: {e}\")\n",
    "        \n",
    "        # Move to next window\n",
    "        start_idx += step_size\n",
    "    \n",
    "    # Convert to DataFrames for analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': all_dates,\n",
    "        'Actual': all_actuals, \n",
    "        'Forecast': all_forecasts\n",
    "    })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(validation_metrics)\n",
    "    \n",
    "    # Overall performance\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_actuals, all_forecasts))\n",
    "    overall_mae = mean_absolute_error(all_actuals, all_forecasts)\n",
    "    \n",
    "    print(f\"\\nüìä WALK-FORWARD VALIDATION RESULTS:\")\n",
    "    print(f\"Total windows: {len(validation_metrics)}\")\n",
    "    print(f\"Overall RMSE: {overall_rmse:.6f}\")\n",
    "    print(f\"Overall MAE: {overall_mae:.6f}\")\n",
    "    print(f\"Average window RMSE: {metrics_df['rmse'].mean():.6f}\")\n",
    "    print(f\"RMSE std deviation: {metrics_df['rmse'].std():.6f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Forecasts vs actuals\n",
    "    axes[0, 0].plot(results_df['Date'], results_df['Actual'], label='Actual', alpha=0.7)\n",
    "    axes[0, 0].plot(results_df['Date'], results_df['Forecast'], label='Forecast', alpha=0.7)\n",
    "    axes[0, 0].set_title('Walk-Forward Forecasts vs Actual')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMSE over time\n",
    "    axes[0, 1].plot(metrics_df['window_end'], metrics_df['rmse'], marker='o')\n",
    "    axes[0, 1].set_title('RMSE Over Time')\n",
    "    axes[0, 1].set_ylabel('RMSE')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = results_df['Actual'] - results_df['Forecast']\n",
    "    axes[1, 0].plot(results_df['Date'], residuals, alpha=0.7)\n",
    "    axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_title('Forecast Residuals')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals distribution\n",
    "    axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[1, 1].set_title('Residuals Distribution')\n",
    "    axes[1, 1].set_xlabel('Residuals')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df, metrics_df\n",
    "\n",
    "# Execute walk-forward validation\n",
    "wf_results, wf_metrics = walk_forward_validation_instructor(\n",
    "    target_series, \n",
    "    model_type='arima', \n",
    "    window_size=200, \n",
    "    step_size=30\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Exercise 2 Complete: Walk-forward validation implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a65fe",
   "metadata": {},
   "source": [
    "## INSTRUCTOR SOLUTION: Exercise 3 - LSTM Hyperparameter Tuning\n",
    "\n",
    "**Teaching Point**: Show systematic hyperparameter optimization and the impact of lookback windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY: Complete solution for Exercise 3\n",
    "def lstm_hyperparameter_tuning_instructor(train_data, test_data, lookback_values=[10, 20, 30, 50]):\n",
    "    \"\"\"Systematic LSTM hyperparameter tuning\"\"\"\n",
    "    \n",
    "    print(f\"üéì INSTRUCTOR SOLUTION: Exercise 3 - LSTM Hyperparameter Tuning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"‚ùå TensorFlow not available. Skipping LSTM tuning.\")\n",
    "        return\n",
    "    \n",
    "    def create_lstm_dataset_instructor(data, lookback):\n",
    "        \"\"\"Create supervised dataset for LSTM\"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(lookback, len(data)):\n",
    "            X.append(data[i-lookback:i])\n",
    "            y.append(data[i])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_and_evaluate_lstm(train_data, test_data, lookback, epochs=30):\n",
    "        \"\"\"Build LSTM with specific lookback and evaluate\"\"\"\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create supervised dataset\n",
    "        X_train, y_train = create_lstm_dataset_instructor(train_scaled, lookback)\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "        \n",
    "        # Build model with early stopping\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=(lookback, 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=0,\n",
    "            shuffle=False,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Generate predictions\n",
    "        full_data = pd.concat([train_data, test_data])\n",
    "        full_scaled = scaler.transform(full_data.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(train_data), len(full_data)):\n",
    "            X_test = full_scaled[i-lookback:i].reshape(1, lookback, 1)\n",
    "            pred_scaled = model.predict(X_test, verbose=0)\n",
    "            pred = scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(test_data, predictions))\n",
    "        mae = mean_absolute_error(test_data, predictions)\n",
    "        \n",
    "        return {\n",
    "            'lookback': lookback,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'predictions': predictions,\n",
    "            'history': history,\n",
    "            'final_val_loss': min(history.history['val_loss'])\n",
    "        }\n",
    "    \n",
    "    # Test different lookback values\n",
    "    results = []\n",
    "    \n",
    "    for lookback in lookback_values:\n",
    "        print(f\"Testing lookback window: {lookback} days...\")\n",
    "        \n",
    "        try:\n",
    "            result = build_and_evaluate_lstm(train_data, test_data, lookback)\n",
    "            results.append(result)\n",
    "            print(f\"  RMSE: {result['rmse']:.6f}, MAE: {result['mae']:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with lookback {lookback}: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No successful LSTM runs\")\n",
    "        return\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_result = min(results, key=lambda x: x['rmse'])\n",
    "    print(f\"\\nüèÜ Best Configuration:\")\n",
    "    print(f\"Lookback: {best_result['lookback']} days\")\n",
    "    print(f\"RMSE: {best_result['rmse']:.6f}\")\n",
    "    print(f\"MAE: {best_result['mae']:.6f}\")\n",
    "    \n",
    "    # Comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Performance comparison\n",
    "    lookbacks = [r['lookback'] for r in results]\n",
    "    rmses = [r['rmse'] for r in results]\n",
    "    maes = [r['mae'] for r in results]\n",
    "    \n",
    "    axes[0, 0].plot(lookbacks, rmses, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_title('RMSE vs Lookback Window')\n",
    "    axes[0, 0].set_xlabel('Lookback Window (days)')\n",
    "    axes[0, 0].set_ylabel('RMSE')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(lookbacks, maes, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_title('MAE vs Lookback Window')\n",
    "    axes[0, 1].set_xlabel('Lookback Window (days)')\n",
    "    axes[0, 1].set_ylabel('MAE')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Best model forecast\n",
    "    axes[0, 2].plot(test_data.index, test_data.values, label='Actual', linewidth=2)\n",
    "    axes[0, 2].plot(test_data.index, best_result['predictions'], \n",
    "                   label=f'LSTM (lookback={best_result[\"lookback\"]})', linewidth=2)\n",
    "    axes[0, 2].set_title('Best LSTM Forecast')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training histories\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    for i, result in enumerate(results[:3]):  # Show first 3 for clarity\n",
    "        axes[1, 0].plot(result['history'].history['loss'], \n",
    "                       color=colors[i], label=f'Lookback {result[\"lookback\"]}')\n",
    "    axes[1, 0].set_title('Training Loss Comparison')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    for i, result in enumerate(results[:3]):\n",
    "        axes[1, 1].plot(result['history'].history['val_loss'], \n",
    "                       color=colors[i], label=f'Lookback {result[\"lookback\"]}')\n",
    "    axes[1, 1].set_title('Validation Loss Comparison')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals analysis for best model\n",
    "    residuals = test_data.values - best_result['predictions']\n",
    "    axes[1, 2].scatter(best_result['predictions'], residuals, alpha=0.6)\n",
    "    axes[1, 2].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1, 2].set_title('Residuals vs Predictions (Best Model)')\n",
    "    axes[1, 2].set_xlabel('Predictions')\n",
    "    axes[1, 2].set_ylabel('Residuals')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Lookback': r['lookback'],\n",
    "            'RMSE': r['rmse'],\n",
    "            'MAE': r['mae'],\n",
    "            'Final_Val_Loss': r['final_val_loss']\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nüìä HYPERPARAMETER TUNING RESULTS:\")\n",
    "    print(results_df.to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    return results, best_result\n",
    "\n",
    "# Execute LSTM hyperparameter tuning\n",
    "if TF_AVAILABLE:\n",
    "    lstm_results, best_lstm = lstm_hyperparameter_tuning_instructor(\n",
    "        train_data, test_data, [10, 20, 30, 50]\n",
    "    )\n",
    "    print(\"‚úÖ Exercise 3 Complete: LSTM hyperparameter tuning completed\")\n",
    "else:\n",
    "    print(\"‚ùå TensorFlow not available - skipping Exercise 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a548725",
   "metadata": {},
   "source": [
    "## INSTRUCTOR NOTE: Teaching Summary\n",
    "\n",
    "**Key Teaching Points for Week 4:**\n",
    "\n",
    "### 1. Model Selection Guidance\n",
    "- **ARIMA**: Best for stationary data with clear autocorrelation patterns\n",
    "- **Prophet**: Excellent for business data with seasonality and missing values  \n",
    "- **LSTM**: Powerful for non-linear patterns but requires large datasets\n",
    "\n",
    "### 2. Common Student Mistakes\n",
    "- Not checking stationarity before ARIMA modeling\n",
    "- Using future information in time series splits (look-ahead bias)\n",
    "- Not scaling data properly for LSTM\n",
    "- Ignoring validation methodology (using random splits instead of temporal)\n",
    "\n",
    "### 3. Practical Implementation Tips\n",
    "- Always implement walk-forward validation for realistic performance assessment\n",
    "- Use multiple metrics (RMSE, MAE, directional accuracy) for comprehensive evaluation\n",
    "- Consider computational costs in real-world applications\n",
    "- Ensemble methods often outperform individual models\n",
    "\n",
    "### 4. Business Applications\n",
    "- Short-term forecasting: ARIMA often sufficient\n",
    "- Medium-term with seasonality: Prophet recommended\n",
    "- Complex patterns with multiple features: LSTM\n",
    "- Risk management: Focus on tail predictions and uncertainty quantification"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
