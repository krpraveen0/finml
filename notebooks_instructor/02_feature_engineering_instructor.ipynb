{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1293554",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Week 2: Data Wrangling & Feature Engineering for Finance\"\n",
    "subtitle: \"INSTRUCTOR VERSION - Complete Solutions\"\n",
    "author: \"Praveen Kumar\"\n",
    "date: 2025-10-08\n",
    "week: 2\n",
    "type: \"instructor_notebook\"\n",
    "version: v1.0\n",
    "---\n",
    "\n",
    "# Week 2: Data Wrangling & Feature Engineering for Finance\n",
    "## **INSTRUCTOR VERSION** üéì\n",
    "\n",
    "This notebook contains **complete solutions** for all exercises. Use this for:\n",
    "- Preparing lecture demonstrations\n",
    "- Providing detailed explanations to students\n",
    "- Checking student work against reference implementations\n",
    "\n",
    "‚ö†Ô∏è **CONFIDENTIAL**: Do not share this version with students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ba76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SETUP - Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Week 2: Feature Engineering for Financial Data\")\n",
    "print(\"üéì INSTRUCTOR VERSION - All solutions included\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef9d52",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading & Initial Exploration\n",
    "\n",
    "**INSTRUCTOR NOTE**: This section demonstrates multiple data loading strategies and comprehensive data validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3728a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR ONLY - Complete data loading with error handling\n",
    "def load_financial_data_instructor(symbol=\"AAPL\", period=\"2y\", fallback=True):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Robust data loading with multiple fallback strategies\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Attempting to download {symbol} data...\")\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"Empty dataset returned\")\n",
    "            \n",
    "        # Convert column names to lowercase for consistency\n",
    "        df.columns = df.columns.str.lower()\n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} rows of {symbol} data\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {symbol}: {e}\")\n",
    "        \n",
    "        if fallback:\n",
    "            print(\"üîÑ Generating synthetic data for demonstration...\")\n",
    "            return generate_synthetic_data_instructor()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def generate_synthetic_data_instructor(n_days=500):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Generate realistic financial data with proper statistical properties\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # Reproducible results\n",
    "    \n",
    "    # Generate dates\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=n_days)\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Generate price series with realistic properties\n",
    "    initial_price = 150.0\n",
    "    returns = np.random.normal(0.0005, 0.02, len(dates))  # Daily returns with slight positive drift\n",
    "    \n",
    "    # Add volatility clustering (GARCH-like behavior)\n",
    "    vol = np.ones(len(dates)) * 0.02\n",
    "    for i in range(1, len(dates)):\n",
    "        vol[i] = 0.05 * vol[i-1] + 0.95 * abs(returns[i-1])\n",
    "    \n",
    "    returns = returns * vol\n",
    "    prices = initial_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Generate OHLC data\n",
    "    highs = prices * (1 + np.abs(np.random.normal(0, 0.01, len(dates))))\n",
    "    lows = prices * (1 - np.abs(np.random.normal(0, 0.01, len(dates))))\n",
    "    opens = np.roll(prices, 1)\n",
    "    opens[0] = initial_price\n",
    "    \n",
    "    # Generate volume with realistic patterns\n",
    "    base_volume = 1000000\n",
    "    volume = base_volume * (1 + np.random.exponential(0.5, len(dates)))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'open': opens,\n",
    "        'high': highs,\n",
    "        'low': lows,\n",
    "        'close': prices,\n",
    "        'volume': volume.astype(int)\n",
    "    }, index=dates)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(df)} days of synthetic data\")\n",
    "    return df\n",
    "\n",
    "# Load data using instructor method\n",
    "df = load_financial_data_instructor(\"AAPL\", \"2y\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d82441",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning & Preprocessing\n",
    "\n",
    "**INSTRUCTOR NOTE**: Comprehensive data quality assessment and cleaning procedures for financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf855ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SOLUTION - Complete data cleaning pipeline\n",
    "def comprehensive_data_cleaning_instructor(df):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Professional-grade data cleaning for financial data\n",
    "    \"\"\"\n",
    "    print(\"üßπ COMPREHENSIVE DATA CLEANING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    cleaned_df = df.copy()\n",
    "    initial_rows = len(cleaned_df)\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    missing_summary = cleaned_df.isnull().sum()\n",
    "    print(\"üìä Missing Values Summary:\")\n",
    "    for col, missing in missing_summary.items():\n",
    "        if missing > 0:\n",
    "            pct = (missing / len(cleaned_df)) * 100\n",
    "            print(f\"  {col}: {missing} ({pct:.2f}%)\")\n",
    "    \n",
    "    # 2. Handle missing values with forward fill (common for financial data)\n",
    "    cleaned_df = cleaned_df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # 3. Validate OHLC relationships\n",
    "    print(\"\\nüîç OHLC Validation:\")\n",
    "    invalid_high = (cleaned_df['high'] < cleaned_df[['open', 'close']].max(axis=1)).sum()\n",
    "    invalid_low = (cleaned_df['low'] > cleaned_df[['open', 'close']].min(axis=1)).sum()\n",
    "    \n",
    "    print(f\"  Invalid high prices: {invalid_high}\")\n",
    "    print(f\"  Invalid low prices: {invalid_low}\")\n",
    "    \n",
    "    # Fix invalid OHLC (adjust highs and lows)\n",
    "    cleaned_df['high'] = np.maximum(cleaned_df['high'], \n",
    "                                   cleaned_df[['open', 'close']].max(axis=1))\n",
    "    cleaned_df['low'] = np.minimum(cleaned_df['low'], \n",
    "                                  cleaned_df[['open', 'close']].min(axis=1))\n",
    "    \n",
    "    # 4. Detect and handle outliers using IQR method\n",
    "    print(\"\\nüìà Outlier Detection (Returns):\")\n",
    "    returns = cleaned_df['close'].pct_change()\n",
    "    Q1 = returns.quantile(0.25)\n",
    "    Q3 = returns.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 3 * IQR  # Using 3*IQR for conservative outlier detection\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    outliers = ((returns < lower_bound) | (returns > upper_bound)) & (~returns.isna())\n",
    "    print(f\"  Outliers detected: {outliers.sum()} ({(outliers.sum()/len(returns))*100:.2f}%)\")\n",
    "    \n",
    "    if outliers.sum() > 0:\n",
    "        print(f\"  Extreme returns range: {returns[outliers].min():.4f} to {returns[outliers].max():.4f}\")\n",
    "        # Cap outliers instead of removing (preserves data continuity)\n",
    "        returns_capped = returns.clip(lower_bound, upper_bound)\n",
    "        outliers_capped = outliers.sum()\n",
    "        print(f\"  Capped {outliers_capped} outlier returns\")\n",
    "    \n",
    "    # 5. Check for suspicious volume patterns\n",
    "    print(\"\\nüìä Volume Analysis:\")\n",
    "    volume_outliers = (cleaned_df['volume'] > cleaned_df['volume'].quantile(0.99)).sum()\n",
    "    zero_volume = (cleaned_df['volume'] == 0).sum()\n",
    "    print(f\"  High volume days (>99th percentile): {volume_outliers}\")\n",
    "    print(f\"  Zero volume days: {zero_volume}\")\n",
    "    \n",
    "    # 6. Final validation\n",
    "    final_rows = len(cleaned_df)\n",
    "    print(f\"\\n‚úÖ Cleaning completed:\")\n",
    "    print(f\"  Rows: {initial_rows} ‚Üí {final_rows}\")\n",
    "    print(f\"  No missing values: {cleaned_df.isnull().sum().sum() == 0}\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# Apply comprehensive cleaning\n",
    "cleaned_data = comprehensive_data_cleaning_instructor(df)\n",
    "print(\"\\nCleaned data sample:\")\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f7651",
   "metadata": {},
   "source": [
    "## Section 3: Basic Feature Engineering\n",
    "\n",
    "**INSTRUCTOR NOTE**: This section covers fundamental financial features with proper implementation details and edge case handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SOLUTION - Complete basic feature engineering\n",
    "def engineer_basic_features_instructor(df):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Comprehensive basic feature engineering\n",
    "    \"\"\"\n",
    "    print(\"‚öôÔ∏è ENGINEERING BASIC FEATURES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create feature DataFrame\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # 1. Returns (multiple types)\n",
    "    print(\"üìà Calculating returns...\")\n",
    "    features_df['returns'] = df['close'].pct_change()\n",
    "    features_df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # Overnight returns (open vs previous close)\n",
    "    features_df['overnight_returns'] = (df['open'] / df['close'].shift(1)) - 1\n",
    "    \n",
    "    # Intraday returns (close vs open)\n",
    "    features_df['intraday_returns'] = (df['close'] / df['open']) - 1\n",
    "    \n",
    "    print(f\"  Returns summary - Mean: {features_df['returns'].mean():.6f}, Std: {features_df['returns'].std():.6f}\")\n",
    "    \n",
    "    # 2. Moving averages (multiple timeframes)\n",
    "    print(\"üìä Calculating moving averages...\")\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        features_df[f'sma_{window}'] = df['close'].rolling(window=window).mean()\n",
    "        features_df[f'ema_{window}'] = df['close'].ewm(span=window).mean()\n",
    "    \n",
    "    # Price relative to moving averages\n",
    "    features_df['price_to_sma20'] = df['close'] / features_df['sma_20']\n",
    "    features_df['price_to_sma50'] = df['close'] / features_df['sma_50']\n",
    "    \n",
    "    # 3. Volatility measures\n",
    "    print(\"üìâ Calculating volatility...\")\n",
    "    for window in [5, 10, 20, 30]:\n",
    "        features_df[f'volatility_{window}'] = features_df['returns'].rolling(window=window).std()\n",
    "        # Annualized volatility\n",
    "        features_df[f'vol_annualized_{window}'] = features_df[f'volatility_{window}'] * np.sqrt(252)\n",
    "    \n",
    "    # Realized volatility (using high-low)\n",
    "    features_df['realized_vol'] = np.log(df['high'] / df['low'])\n",
    "    \n",
    "    # 4. Volume features\n",
    "    print(\"üìä Engineering volume features...\")\n",
    "    features_df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    features_df['volume_ratio'] = df['volume'] / features_df['volume_sma_20']\n",
    "    \n",
    "    # Volume-price trend\n",
    "    features_df['vpt'] = (df['volume'] * ((df['close'] - df['close'].shift(1)) / df['close'].shift(1))).cumsum()\n",
    "    \n",
    "    # 5. Price gaps\n",
    "    print(\"üîÑ Calculating price gaps...\")\n",
    "    features_df['gap'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "    features_df['gap_up'] = (features_df['gap'] > 0.01).astype(int)  # Gap up > 1%\n",
    "    features_df['gap_down'] = (features_df['gap'] < -0.01).astype(int)  # Gap down > 1%\n",
    "    \n",
    "    # 6. High-Low ranges\n",
    "    print(\"üìè Calculating price ranges...\")\n",
    "    features_df['daily_range'] = (df['high'] - df['low']) / df['close']\n",
    "    features_df['body_size'] = abs(df['close'] - df['open']) / df['close']\n",
    "    features_df['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / df['close']\n",
    "    features_df['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / df['close']\n",
    "    \n",
    "    print(f\"‚úÖ Basic features engineered: {len([col for col in features_df.columns if col not in df.columns])} new features\")\n",
    "    return features_df\n",
    "\n",
    "# Apply basic feature engineering\n",
    "df_with_basic_features = engineer_basic_features_instructor(cleaned_data)\n",
    "\n",
    "# Display feature summary\n",
    "new_features = [col for col in df_with_basic_features.columns if col not in cleaned_data.columns]\n",
    "print(f\"\\nNew features created ({len(new_features)}):\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nFeature DataFrame shape: {df_with_basic_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7944b7",
   "metadata": {},
   "source": [
    "## Section 4: Advanced Technical Indicators\n",
    "\n",
    "**INSTRUCTOR NOTE**: Implementation of sophisticated technical analysis indicators with proper parameter optimization and interpretation guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SOLUTION - Complete technical indicators implementation\n",
    "def calculate_advanced_indicators_instructor(df):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Professional technical indicators with optimized parameters\n",
    "    \"\"\"\n",
    "    print(\"üîß ADVANCED TECHNICAL INDICATORS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    tech_df = df.copy()\n",
    "    \n",
    "    # 1. RSI with proper implementation\n",
    "    print(\"üìä Calculating RSI...\")\n",
    "    def calculate_rsi_instructor(prices, window=14):\n",
    "        delta = prices.diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        \n",
    "        # Use Wilder's smoothing (EMA with alpha = 1/window)\n",
    "        avg_gain = gain.ewm(alpha=1/window, adjust=False).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/window, adjust=False).mean()\n",
    "        \n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    tech_df['rsi'] = calculate_rsi_instructor(df['close'])\n",
    "    tech_df['rsi_overbought'] = (tech_df['rsi'] > 70).astype(int)\n",
    "    tech_df['rsi_oversold'] = (tech_df['rsi'] < 30).astype(int)\n",
    "    \n",
    "    # 2. MACD with signal and histogram\n",
    "    print(\"üìà Calculating MACD...\")\n",
    "    def calculate_macd_instructor(prices, fast=12, slow=26, signal=9):\n",
    "        ema_fast = prices.ewm(span=fast).mean()\n",
    "        ema_slow = prices.ewm(span=slow).mean()\n",
    "        macd_line = ema_fast - ema_slow\n",
    "        signal_line = macd_line.ewm(span=signal).mean()\n",
    "        histogram = macd_line - signal_line\n",
    "        return macd_line, signal_line, histogram\n",
    "    \n",
    "    tech_df['macd'], tech_df['macd_signal'], tech_df['macd_histogram'] = calculate_macd_instructor(df['close'])\n",
    "    tech_df['macd_bullish'] = (tech_df['macd'] > tech_df['macd_signal']).astype(int)\n",
    "    tech_df['macd_crossover'] = ((tech_df['macd'] > tech_df['macd_signal']) & \n",
    "                                (tech_df['macd'].shift(1) <= tech_df['macd_signal'].shift(1))).astype(int)\n",
    "    \n",
    "    # 3. Bollinger Bands with squeeze detection\n",
    "    print(\"üìä Calculating Bollinger Bands...\")\n",
    "    def calculate_bollinger_bands_instructor(prices, window=20, num_std=2):\n",
    "        sma = prices.rolling(window=window).mean()\n",
    "        std = prices.rolling(window=window).std()\n",
    "        upper = sma + (num_std * std)\n",
    "        lower = sma - (num_std * std)\n",
    "        \n",
    "        # Additional metrics\n",
    "        width = (upper - lower) / sma\n",
    "        position = (prices - lower) / (upper - lower)\n",
    "        squeeze = width < width.rolling(100).quantile(0.1)  # Bottom 10% of width\n",
    "        \n",
    "        return upper, sma, lower, width, position, squeeze\n",
    "    \n",
    "    (tech_df['bb_upper'], tech_df['bb_middle'], tech_df['bb_lower'], \n",
    "     tech_df['bb_width'], tech_df['bb_position'], tech_df['bb_squeeze']) = calculate_bollinger_bands_instructor(df['close'])\n",
    "    \n",
    "    # Band breakouts\n",
    "    tech_df['bb_breakout_upper'] = (df['close'] > tech_df['bb_upper']).astype(int)\n",
    "    tech_df['bb_breakout_lower'] = (df['close'] < tech_df['bb_lower']).astype(int)\n",
    "    \n",
    "    # 4. Stochastic Oscillator\n",
    "    print(\"üéØ Calculating Stochastic...\")\n",
    "    def calculate_stochastic_instructor(high, low, close, k_window=14, d_window=3):\n",
    "        lowest_low = low.rolling(window=k_window).min()\n",
    "        highest_high = high.rolling(window=k_window).max()\n",
    "        k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))\n",
    "        d_percent = k_percent.rolling(window=d_window).mean()\n",
    "        return k_percent, d_percent\n",
    "    \n",
    "    tech_df['stoch_k'], tech_df['stoch_d'] = calculate_stochastic_instructor(df['high'], df['low'], df['close'])\n",
    "    tech_df['stoch_overbought'] = (tech_df['stoch_k'] > 80).astype(int)\n",
    "    tech_df['stoch_oversold'] = (tech_df['stoch_k'] < 20).astype(int)\n",
    "    \n",
    "    # 5. Williams %R\n",
    "    print(\"üîÑ Calculating Williams %R...\")\n",
    "    def calculate_williams_r_instructor(high, low, close, window=14):\n",
    "        highest_high = high.rolling(window=window).max()\n",
    "        lowest_low = low.rolling(window=window).min()\n",
    "        williams_r = -100 * ((highest_high - close) / (highest_high - lowest_low))\n",
    "        return williams_r\n",
    "    \n",
    "    tech_df['williams_r'] = calculate_williams_r_instructor(df['high'], df['low'], df['close'])\n",
    "    \n",
    "    # 6. Average True Range (ATR)\n",
    "    print(\"üìè Calculating ATR...\")\n",
    "    def calculate_atr_instructor(high, low, close, window=14):\n",
    "        tr1 = high - low\n",
    "        tr2 = abs(high - close.shift(1))\n",
    "        tr3 = abs(low - close.shift(1))\n",
    "        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = true_range.rolling(window=window).mean()\n",
    "        return atr\n",
    "    \n",
    "    tech_df['atr'] = calculate_atr_instructor(df['high'], df['low'], df['close'])\n",
    "    tech_df['atr_normalized'] = tech_df['atr'] / df['close']  # ATR as % of price\n",
    "    \n",
    "    print(f\"‚úÖ Advanced indicators calculated: {len([col for col in tech_df.columns if col not in df.columns])} new features\")\n",
    "    return tech_df\n",
    "\n",
    "# Apply advanced technical indicators\n",
    "df_with_indicators = calculate_advanced_indicators_instructor(df_with_basic_features)\n",
    "\n",
    "# Summary of indicator signals\n",
    "latest = df_with_indicators.iloc[-1]\n",
    "print(f\"\\nüìä CURRENT TECHNICAL SIGNALS:\")\n",
    "print(f\"RSI: {latest['rsi']:.1f} ({'Overbought' if latest['rsi'] > 70 else 'Oversold' if latest['rsi'] < 30 else 'Neutral'})\")\n",
    "print(f\"MACD: {'Bullish' if latest['macd_bullish'] else 'Bearish'}\")\n",
    "print(f\"BB Position: {latest['bb_position']:.3f} ({'Above upper' if latest['bb_position'] > 1 else 'Below lower' if latest['bb_position'] < 0 else 'Within bands'})\")\n",
    "print(f\"Stochastic: {latest['stoch_k']:.1f} ({'Overbought' if latest['stoch_k'] > 80 else 'Oversold' if latest['stoch_k'] < 20 else 'Neutral'})\")\n",
    "\n",
    "technical_features = [col for col in df_with_indicators.columns if col not in df_with_basic_features.columns]\n",
    "print(f\"\\nTechnical features added: {len(technical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac209975",
   "metadata": {},
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "**INSTRUCTOR NOTE**: Complete solutions for all student exercises with detailed explanations and alternative approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b97536",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution: Bollinger Bands Implementation\n",
    "\n",
    "**INSTRUCTOR SOLUTION** with comprehensive analysis and trading applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ec9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SOLUTION - Exercise 2: Complete Bollinger Bands\n",
    "def bollinger_bands_complete_solution(prices, window=20, num_std=2):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Professional Bollinger Bands implementation\n",
    "    with additional trading metrics and signal generation\n",
    "    \"\"\"\n",
    "    # Core calculations\n",
    "    middle_band = prices.rolling(window=window).mean()\n",
    "    rolling_std = prices.rolling(window=window).std()\n",
    "    upper_band = middle_band + (num_std * rolling_std)\n",
    "    lower_band = middle_band - (num_std * rolling_std)\n",
    "    \n",
    "    # Advanced metrics\n",
    "    band_width = (upper_band - lower_band) / middle_band\n",
    "    price_position = (prices - lower_band) / (upper_band - lower_band)\n",
    "    \n",
    "    # Trading signals\n",
    "    squeeze_threshold = band_width.rolling(100).quantile(0.1)\n",
    "    squeeze = band_width < squeeze_threshold\n",
    "    \n",
    "    # Breakout detection\n",
    "    upper_breakout = prices > upper_band\n",
    "    lower_breakout = prices < lower_band\n",
    "    \n",
    "    # Mean reversion signals\n",
    "    buy_signal = (price_position < 0.1) & (price_position.shift(1) >= 0.1)\n",
    "    sell_signal = (price_position > 0.9) & (price_position.shift(1) <= 0.9)\n",
    "    \n",
    "    return {\n",
    "        'upper': upper_band,\n",
    "        'middle': middle_band, \n",
    "        'lower': lower_band,\n",
    "        'width': band_width,\n",
    "        'position': price_position,\n",
    "        'squeeze': squeeze,\n",
    "        'upper_breakout': upper_breakout,\n",
    "        'lower_breakout': lower_breakout,\n",
    "        'buy_signal': buy_signal,\n",
    "        'sell_signal': sell_signal\n",
    "    }\n",
    "\n",
    "# Apply complete solution\n",
    "bb_solution = bollinger_bands_complete_solution(df_with_indicators['close'])\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 20))\n",
    "\n",
    "# 1. Price with Bollinger Bands and signals\n",
    "axes[0].plot(df_with_indicators.index, df_with_indicators['close'], 'k-', linewidth=2, label='Close Price')\n",
    "axes[0].plot(df_with_indicators.index, bb_solution['upper'], 'r--', alpha=0.7, label='Upper Band')\n",
    "axes[0].plot(df_with_indicators.index, bb_solution['middle'], 'b-', alpha=0.7, label='Middle Band')\n",
    "axes[0].plot(df_with_indicators.index, bb_solution['lower'], 'g--', alpha=0.7, label='Lower Band')\n",
    "axes[0].fill_between(df_with_indicators.index, bb_solution['upper'], bb_solution['lower'], alpha=0.1, color='gray')\n",
    "\n",
    "# Highlight trading signals\n",
    "buy_dates = df_with_indicators.index[bb_solution['buy_signal']]\n",
    "sell_dates = df_with_indicators.index[bb_solution['sell_signal']]\n",
    "if len(buy_dates) > 0:\n",
    "    axes[0].scatter(buy_dates, df_with_indicators.loc[buy_dates, 'close'], \n",
    "                   color='green', marker='^', s=100, label='Buy Signal', zorder=5)\n",
    "if len(sell_dates) > 0:\n",
    "    axes[0].scatter(sell_dates, df_with_indicators.loc[sell_dates, 'close'], \n",
    "                   color='red', marker='v', s=100, label='Sell Signal', zorder=5)\n",
    "\n",
    "axes[0].set_title('Bollinger Bands with Trading Signals')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Band Width with squeeze periods\n",
    "axes[1].plot(df_with_indicators.index, bb_solution['width'], 'purple', linewidth=2)\n",
    "axes[1].fill_between(df_with_indicators.index, 0, bb_solution['width'], \n",
    "                    where=bb_solution['squeeze'], alpha=0.3, color='red', label='Squeeze Periods')\n",
    "axes[1].set_title('Bollinger Band Width (Volatility Measure)')\n",
    "axes[1].set_ylabel('Band Width')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Price Position\n",
    "axes[2].plot(df_with_indicators.index, bb_solution['position'], 'orange', linewidth=2)\n",
    "axes[2].axhline(y=0.5, color='blue', linestyle='-', alpha=0.5)\n",
    "axes[2].axhline(y=0.8, color='red', linestyle='--', alpha=0.7)\n",
    "axes[2].axhline(y=0.2, color='green', linestyle='--', alpha=0.7)\n",
    "axes[2].fill_between(df_with_indicators.index, 0.8, 1.2, alpha=0.1, color='red', label='Overbought Zone')\n",
    "axes[2].fill_between(df_with_indicators.index, -0.2, 0.2, alpha=0.1, color='green', label='Oversold Zone')\n",
    "axes[2].set_title('Price Position Within Bands')\n",
    "axes[2].set_ylabel('Position (0=Lower, 1=Upper)')\n",
    "axes[2].set_ylim(-0.2, 1.2)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Breakout Analysis\n",
    "breakout_data = pd.DataFrame({\n",
    "    'Upper Breakouts': bb_solution['upper_breakout'].astype(int),\n",
    "    'Lower Breakouts': bb_solution['lower_breakout'].astype(int)\n",
    "}, index=df_with_indicators.index)\n",
    "\n",
    "axes[3].plot(df_with_indicators.index, breakout_data['Upper Breakouts'], 'r-', alpha=0.7, label='Upper Breakouts')\n",
    "axes[3].plot(df_with_indicators.index, -breakout_data['Lower Breakouts'], 'g-', alpha=0.7, label='Lower Breakouts')\n",
    "axes[3].fill_between(df_with_indicators.index, 0, breakout_data['Upper Breakouts'], alpha=0.3, color='red')\n",
    "axes[3].fill_between(df_with_indicators.index, 0, -breakout_data['Lower Breakouts'], alpha=0.3, color='green')\n",
    "axes[3].set_title('Band Breakout Events')\n",
    "axes[3].set_ylabel('Breakout Direction')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribution Analysis\n",
    "axes[4].hist(bb_solution['position'].dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[4].axvline(x=0.5, color='blue', linestyle='-', alpha=0.7, label='Middle (50%)')\n",
    "axes[4].axvline(x=0.8, color='red', linestyle='--', alpha=0.7, label='80% Threshold')\n",
    "axes[4].axvline(x=0.2, color='green', linestyle='--', alpha=0.7, label='20% Threshold')\n",
    "axes[4].set_title('Distribution of Price Position')\n",
    "axes[4].set_xlabel('Position within Bands')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "axes[4].legend()\n",
    "axes[4].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance Analysis\n",
    "print(\"üéØ BOLLINGER BANDS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Upper breakouts: {bb_solution['upper_breakout'].sum()} ({(bb_solution['upper_breakout'].sum()/len(df_with_indicators))*100:.2f}%)\")\n",
    "print(f\"Lower breakouts: {bb_solution['lower_breakout'].sum()} ({(bb_solution['lower_breakout'].sum()/len(df_with_indicators))*100:.2f}%)\")\n",
    "print(f\"Squeeze periods: {bb_solution['squeeze'].sum()} days ({(bb_solution['squeeze'].sum()/len(df_with_indicators))*100:.2f}%)\")\n",
    "print(f\"Buy signals: {bb_solution['buy_signal'].sum()}\")\n",
    "print(f\"Sell signals: {bb_solution['sell_signal'].sum()}\")\n",
    "print(f\"Average band width: {bb_solution['width'].mean():.4f}\")\n",
    "\n",
    "# Position statistics\n",
    "pos_stats = bb_solution['position'].describe()\n",
    "print(f\"\\nPrice Position Statistics:\")\n",
    "for stat, value in pos_stats.items():\n",
    "    print(f\"  {stat}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1acc8",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution: PCA vs Feature Importance\n",
    "\n",
    "**INSTRUCTOR SOLUTION** with comprehensive comparison and practical recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SOLUTION - Exercise 3: Complete PCA vs Feature Importance Analysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "def complete_pca_vs_importance_analysis(df):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Comprehensive PCA vs Feature Importance comparison\n",
    "    with model performance evaluation\n",
    "    \"\"\"\n",
    "    print(\"üî¨ COMPLETE PCA vs FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare comprehensive feature set\n",
    "    feature_columns = [col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume']]\n",
    "    features_df = df[feature_columns].dropna()\n",
    "    \n",
    "    print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "    print(f\"Features analyzed: {len(feature_columns)}\")\n",
    "    \n",
    "    # Target variable for importance ranking\n",
    "    target = features_df['returns'].shift(-1).dropna()\n",
    "    X = features_df.iloc[:-1]  # Remove last row to match target\n",
    "    \n",
    "    # 1. COMPREHENSIVE PCA ANALYSIS\n",
    "    print(f\"\\n{'='*20} PCA ANALYSIS {'='*20}\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA()\n",
    "    pca_components = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Explained variance analysis\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "    \n",
    "    # Find components for different variance thresholds\n",
    "    components_80 = np.argmax(cumsum_var >= 0.80) + 1\n",
    "    components_90 = np.argmax(cumsum_var >= 0.90) + 1\n",
    "    components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "    \n",
    "    print(f\"Components for 80% variance: {components_80}\")\n",
    "    print(f\"Components for 90% variance: {components_90}\")\n",
    "    print(f\"Components for 95% variance: {components_95}\")\n",
    "    \n",
    "    # Get detailed loadings\n",
    "    n_components_detail = min(5, len(feature_columns))\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_[:n_components_detail].T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components_detail)],\n",
    "        index=X.columns\n",
    "    )\n",
    "    \n",
    "    # 2. RANDOM FOREST FEATURE IMPORTANCE\n",
    "    print(f\"\\n{'='*15} RANDOM FOREST ANALYSIS {'='*15}\")\n",
    "    \n",
    "    # Multiple RF models with different parameters\n",
    "    rf_results = {}\n",
    "    for n_est, max_d in [(50, 5), (100, 10), (200, 15)]:\n",
    "        rf = RandomForestRegressor(n_estimators=n_est, max_depth=max_d, \n",
    "                                 random_state=42, n_jobs=-1)\n",
    "        rf.fit(X, target)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        rf_results[f'RF_{n_est}_{max_d}'] = importance_df\n",
    "    \n",
    "    # Use the middle configuration for main analysis\n",
    "    main_rf_result = rf_results['RF_100_10']\n",
    "    \n",
    "    # 3. CORRELATION ANALYSIS\n",
    "    print(f\"\\n{'='*15} CORRELATION ANALYSIS {'='*15}\")\n",
    "    \n",
    "    # Calculate correlations between different rankings\n",
    "    correlations = {}\n",
    "    \n",
    "    for pc in [f'PC{i+1}' for i in range(min(3, n_components_detail))]:\n",
    "        pc_loadings_abs = loadings[pc].abs().sort_values(ascending=False)\n",
    "        \n",
    "        # Align with RF importance\n",
    "        rf_aligned = main_rf_result.set_index('feature')['importance']\n",
    "        common_features = pc_loadings_abs.index.intersection(rf_aligned.index)\n",
    "        \n",
    "        if len(common_features) > 0:\n",
    "            corr = pc_loadings_abs[common_features].corr(rf_aligned[common_features])\n",
    "            correlations[f'{pc}_vs_RF'] = corr\n",
    "    \n",
    "    # 4. MODEL PERFORMANCE COMPARISON\n",
    "    print(f\"\\n{'='*15} MODEL PERFORMANCE {'='*15}\")\n",
    "    \n",
    "    # Split data for evaluation\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = target.iloc[:split_idx], target.iloc[split_idx:]\n",
    "    \n",
    "    # Scale for PCA\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    performance_results = {}\n",
    "    \n",
    "    # Original features\n",
    "    rf_orig = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf_orig.fit(X_train, y_train)\n",
    "    y_pred_orig = rf_orig.predict(X_test)\n",
    "    performance_results['Original Features'] = {\n",
    "        'R2': r2_score(y_test, y_pred_orig),\n",
    "        'MSE': mean_squared_error(y_test, y_pred_orig),\n",
    "        'Features': len(X.columns)\n",
    "    }\n",
    "    \n",
    "    # PCA features\n",
    "    for n_comp in [components_80, components_90, components_95]:\n",
    "        pca_model = PCA(n_components=n_comp)\n",
    "        X_train_pca = pca_model.fit_transform(X_train_scaled)\n",
    "        X_test_pca = pca_model.transform(X_test_scaled)\n",
    "        \n",
    "        rf_pca = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "        rf_pca.fit(X_train_pca, y_train)\n",
    "        y_pred_pca = rf_pca.predict(X_test_pca)\n",
    "        \n",
    "        var_explained = sum(pca_model.explained_variance_ratio_)\n",
    "        performance_results[f'PCA_{n_comp}_components'] = {\n",
    "            'R2': r2_score(y_test, y_pred_pca),\n",
    "            'MSE': mean_squared_error(y_test, y_pred_pca),\n",
    "            'Features': n_comp,\n",
    "            'Variance_Explained': var_explained\n",
    "        }\n",
    "    \n",
    "    # Top RF features\n",
    "    for top_n in [10, 15, 20]:\n",
    "        if top_n <= len(main_rf_result):\n",
    "            top_features = main_rf_result.head(top_n)['feature'].tolist()\n",
    "            X_train_top = X_train[top_features]\n",
    "            X_test_top = X_test[top_features]\n",
    "            \n",
    "            rf_top = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "            rf_top.fit(X_train_top, y_train)\n",
    "            y_pred_top = rf_top.predict(X_test_top)\n",
    "            \n",
    "            performance_results[f'Top_{top_n}_RF_features'] = {\n",
    "                'R2': r2_score(y_test, y_pred_top),\n",
    "                'MSE': mean_squared_error(y_test, y_pred_top),\n",
    "                'Features': top_n\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'pca_results': {\n",
    "            'explained_variance': explained_var,\n",
    "            'cumsum_variance': cumsum_var,\n",
    "            'loadings': loadings,\n",
    "            'components_80': components_80,\n",
    "            'components_90': components_90,\n",
    "            'components_95': components_95\n",
    "        },\n",
    "        'rf_results': rf_results,\n",
    "        'correlations': correlations,\n",
    "        'performance': performance_results,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "\n",
    "# Execute complete analysis\n",
    "analysis_results = complete_pca_vs_importance_analysis(df_with_indicators)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "# 1. PCA Explained Variance\n",
    "pca_res = analysis_results['pca_results']\n",
    "axes[0, 0].bar(range(1, min(21, len(pca_res['explained_variance'])) + 1), \n",
    "               pca_res['explained_variance'][:20], alpha=0.7, color='steelblue')\n",
    "axes[0, 0].plot(range(1, min(21, len(pca_res['cumsum_variance'])) + 1), \n",
    "                pca_res['cumsum_variance'][:20], 'ro-', markersize=4)\n",
    "axes[0, 0].axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='80%')\n",
    "axes[0, 0].axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90%')\n",
    "axes[0, 0].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%')\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0, 0].set_title('PCA Explained Variance')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature Importance\n",
    "main_rf = analysis_results['rf_results']['RF_100_10']\n",
    "top_features = main_rf.head(15)\n",
    "axes[0, 1].barh(range(len(top_features)), top_features['importance'], color='forestgreen', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(top_features)))\n",
    "axes[0, 1].set_yticklabels(top_features['feature'])\n",
    "axes[0, 1].set_xlabel('Importance Score')\n",
    "axes[0, 1].set_title('Random Forest Feature Importance (Top 15)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. PCA Loadings Heatmap\n",
    "sns.heatmap(pca_res['loadings'].T, annot=False, cmap='RdBu_r', center=0, \n",
    "            ax=axes[1, 0], cbar_kws={'label': 'Loading'})\n",
    "axes[1, 0].set_title('PCA Loadings Matrix')\n",
    "axes[1, 0].set_xlabel('Features')\n",
    "\n",
    "# 4. Performance Comparison\n",
    "perf_data = analysis_results['performance']\n",
    "methods = list(perf_data.keys())\n",
    "r2_scores = [perf_data[method]['R2'] for method in methods]\n",
    "feature_counts = [perf_data[method]['Features'] for method in methods]\n",
    "\n",
    "scatter = axes[1, 1].scatter(feature_counts, r2_scores, s=100, alpha=0.7, c=range(len(methods)), cmap='viridis')\n",
    "for i, method in enumerate(methods):\n",
    "    axes[1, 1].annotate(method, (feature_counts[i], r2_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1, 1].set_xlabel('Number of Features')\n",
    "axes[1, 1].set_ylabel('R¬≤ Score')\n",
    "axes[1, 1].set_title('Model Performance vs Feature Count')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Correlation Analysis\n",
    "if analysis_results['correlations']:\n",
    "    corr_data = analysis_results['correlations']\n",
    "    corr_names = list(corr_data.keys())\n",
    "    corr_values = list(corr_data.values())\n",
    "    \n",
    "    axes[2, 0].bar(corr_names, corr_values, alpha=0.7, color='purple')\n",
    "    axes[2, 0].set_ylabel('Correlation Coefficient')\n",
    "    axes[2, 0].set_title('PCA Loadings vs RF Importance Correlation')\n",
    "    axes[2, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Feature Reduction Efficiency\n",
    "methods_subset = [m for m in methods if 'PCA' in m or 'Top_' in m]\n",
    "efficiency_data = []\n",
    "for method in methods_subset:\n",
    "    perf = perf_data[method]\n",
    "    efficiency = perf['R2'] / perf['Features']  # R¬≤ per feature\n",
    "    efficiency_data.append((method, efficiency, perf['Features'], perf['R2']))\n",
    "\n",
    "efficiency_data.sort(key=lambda x: x[1], reverse=True)\n",
    "method_names = [x[0] for x in efficiency_data]\n",
    "efficiency_scores = [x[1] for x in efficiency_data]\n",
    "\n",
    "axes[2, 1].bar(range(len(method_names)), efficiency_scores, alpha=0.7, color='coral')\n",
    "axes[2, 1].set_xticks(range(len(method_names)))\n",
    "axes[2, 1].set_xticklabels(method_names, rotation=45, ha='right')\n",
    "axes[2, 1].set_ylabel('R¬≤ per Feature')\n",
    "axes[2, 1].set_title('Feature Reduction Efficiency')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed Results Summary\n",
    "print(f\"\\n{'='*20} ANALYSIS SUMMARY {'='*20}\")\n",
    "print(f\"Total features analyzed: {len(analysis_results['feature_columns'])}\")\n",
    "print(f\"PCA components for 95% variance: {pca_res['components_95']}\")\n",
    "print(f\"Variance reduction: {len(analysis_results['feature_columns'])} ‚Üí {pca_res['components_95']} features\")\n",
    "\n",
    "print(f\"\\nTop 5 most important features (RF):\")\n",
    "for i, row in main_rf.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "best_performance = max(perf_data.items(), key=lambda x: x[1]['R2'])\n",
    "print(f\"Best R¬≤: {best_performance[1]['R2']:.4f} ({best_performance[0]})\")\n",
    "\n",
    "most_efficient = max([(k, v['R2']/v['Features']) for k, v in perf_data.items()], key=lambda x: x[1])\n",
    "print(f\"Most efficient: {most_efficient[1]:.6f} R¬≤/feature ({most_efficient[0]})\")\n",
    "\n",
    "print(f\"\\nRECOMMENDATIONS:\")\n",
    "if pca_res['components_95'] < len(analysis_results['feature_columns']) * 0.5:\n",
    "    print(f\"‚úÖ PCA recommended: Reduces {len(analysis_results['feature_columns'])} features to {pca_res['components_95']} with minimal information loss\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Feature selection may be better: PCA doesn't provide significant dimensionality reduction\")\n",
    "\n",
    "print(f\"üéØ Exercise 3 Complete: Comprehensive PCA vs Feature Importance analysis finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315278f",
   "metadata": {},
   "source": [
    "## Week 2 Challenge: Volatility Regime Detection\n",
    "\n",
    "**INSTRUCTOR NOTE**: This advanced challenge introduces regime-switching models and volatility forecasting - preparation for Week 3 content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR SOLUTION - Advanced Challenge: Volatility Regime Detection\n",
    "def volatility_regime_detection_instructor(df, lookback=30):\n",
    "    \"\"\"\n",
    "    INSTRUCTOR SOLUTION: Advanced volatility regime detection using multiple methods\n",
    "    \"\"\"\n",
    "    print(\"üéØ ADVANCED CHALLENGE: VOLATILITY REGIME DETECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate various volatility measures\n",
    "    returns = df['close'].pct_change()\n",
    "    \n",
    "    # 1. Rolling volatility (multiple windows)\n",
    "    vol_5d = returns.rolling(5).std() * np.sqrt(252)\n",
    "    vol_20d = returns.rolling(20).std() * np.sqrt(252)\n",
    "    vol_60d = returns.rolling(60).std() * np.sqrt(252)\n",
    "    \n",
    "    # 2. GARCH-style volatility (simplified)\n",
    "    vol_garch = returns.ewm(alpha=0.06).std() * np.sqrt(252)\n",
    "    \n",
    "    # 3. Realized volatility using OHLC\n",
    "    realized_vol = np.log(df['high'] / df['low']) * np.sqrt(252)\n",
    "    \n",
    "    # 4. Regime detection using quantiles\n",
    "    vol_primary = vol_20d  # Use 20-day as primary measure\n",
    "    \n",
    "    # Define regimes based on rolling quantiles\n",
    "    low_threshold = vol_primary.rolling(lookback*5).quantile(0.33)\n",
    "    high_threshold = vol_primary.rolling(lookback*5).quantile(0.67)\n",
    "    \n",
    "    # Create regime indicators\n",
    "    regimes = pd.Series(index=df.index, dtype=int)\n",
    "    regimes = regimes.fillna(1)  # Default to medium volatility\n",
    "    \n",
    "    regimes[vol_primary <= low_threshold] = 0  # Low volatility\n",
    "    regimes[vol_primary >= high_threshold] = 2  # High volatility\n",
    "    \n",
    "    # 5. Regime transition analysis\n",
    "    regime_changes = regimes.diff().abs() > 0\n",
    "    transitions = regimes[regime_changes]\n",
    "    \n",
    "    # 6. Persistence analysis\n",
    "    regime_runs = []\n",
    "    current_regime = regimes.iloc[0] if not pd.isna(regimes.iloc[0]) else 1\n",
    "    run_length = 1\n",
    "    \n",
    "    for i in range(1, len(regimes)):\n",
    "        if regimes.iloc[i] == current_regime:\n",
    "            run_length += 1\n",
    "        else:\n",
    "            regime_runs.append((current_regime, run_length))\n",
    "            current_regime = regimes.iloc[i]\n",
    "            run_length = 1\n",
    "    \n",
    "    # Add final run\n",
    "    regime_runs.append((current_regime, run_length))\n",
    "    \n",
    "    # Calculate average persistence\n",
    "    avg_persistence = {}\n",
    "    for regime in [0, 1, 2]:\n",
    "        regime_lengths = [length for reg, length in regime_runs if reg == regime]\n",
    "        avg_persistence[regime] = np.mean(regime_lengths) if regime_lengths else 0\n",
    "    \n",
    "    return {\n",
    "        'volatilities': {\n",
    "            'vol_5d': vol_5d,\n",
    "            'vol_20d': vol_20d,\n",
    "            'vol_60d': vol_60d,\n",
    "            'vol_garch': vol_garch,\n",
    "            'realized_vol': realized_vol\n",
    "        },\n",
    "        'regimes': regimes,\n",
    "        'thresholds': {\n",
    "            'low': low_threshold,\n",
    "            'high': high_threshold\n",
    "        },\n",
    "        'transitions': transitions,\n",
    "        'persistence': avg_persistence,\n",
    "        'regime_runs': regime_runs\n",
    "    }\n",
    "\n",
    "# Apply volatility regime detection\n",
    "regime_results = volatility_regime_detection_instructor(df_with_indicators)\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 16))\n",
    "\n",
    "# 1. Price with regime background\n",
    "regimes = regime_results['regimes']\n",
    "axes[0].plot(df_with_indicators.index, df_with_indicators['close'], 'k-', linewidth=2, label='Close Price')\n",
    "\n",
    "# Color background by regime\n",
    "for regime, color, alpha, label in [(0, 'green', 0.2, 'Low Vol'), (1, 'yellow', 0.1, 'Medium Vol'), (2, 'red', 0.2, 'High Vol')]:\n",
    "    regime_mask = regimes == regime\n",
    "    if regime_mask.any():\n",
    "        # Create continuous segments for background\n",
    "        regime_starts = regime_mask & (~regime_mask.shift(1).fillna(False))\n",
    "        regime_ends = regime_mask & (~regime_mask.shift(-1).fillna(False))\n",
    "        \n",
    "        for start_idx in df_with_indicators.index[regime_starts]:\n",
    "            # Find corresponding end\n",
    "            end_candidates = df_with_indicators.index[regime_ends & (df_with_indicators.index >= start_idx)]\n",
    "            if len(end_candidates) > 0:\n",
    "                end_idx = end_candidates[0]\n",
    "                axes[0].axvspan(start_idx, end_idx, alpha=alpha, color=color, label=label if start_idx == df_with_indicators.index[regime_starts][0] else \"\")\n",
    "\n",
    "axes[0].set_title('Stock Price with Volatility Regimes')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Multiple volatility measures\n",
    "vols = regime_results['volatilities']\n",
    "axes[1].plot(df_with_indicators.index, vols['vol_5d'], label='5-day Vol', alpha=0.7)\n",
    "axes[1].plot(df_with_indicators.index, vols['vol_20d'], label='20-day Vol', linewidth=2)\n",
    "axes[1].plot(df_with_indicators.index, vols['vol_60d'], label='60-day Vol', alpha=0.7)\n",
    "axes[1].plot(df_with_indicators.index, vols['vol_garch'], label='GARCH-style Vol', linestyle='--')\n",
    "\n",
    "# Add regime thresholds\n",
    "thresholds = regime_results['thresholds']\n",
    "axes[1].plot(df_with_indicators.index, thresholds['low'], 'g--', alpha=0.7, label='Low Threshold')\n",
    "axes[1].plot(df_with_indicators.index, thresholds['high'], 'r--', alpha=0.7, label='High Threshold')\n",
    "\n",
    "axes[1].set_title('Volatility Measures and Regime Thresholds')\n",
    "axes[1].set_ylabel('Annualized Volatility')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Regime time series\n",
    "regime_colors = {0: 'green', 1: 'yellow', 2: 'red'}\n",
    "regime_labels = {0: 'Low Volatility', 1: 'Medium Volatility', 2: 'High Volatility'}\n",
    "\n",
    "for regime in [0, 1, 2]:\n",
    "    regime_mask = regimes == regime\n",
    "    axes[2].scatter(df_with_indicators.index[regime_mask], [regime]*regime_mask.sum(), \n",
    "                   c=regime_colors[regime], alpha=0.6, s=20, label=regime_labels[regime])\n",
    "\n",
    "axes[2].set_title('Volatility Regime Classification Over Time')\n",
    "axes[2].set_ylabel('Regime')\n",
    "axes[2].set_yticks([0, 1, 2])\n",
    "axes[2].set_yticklabels(['Low', 'Medium', 'High'])\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Regime statistics\n",
    "regime_counts = regimes.value_counts().sort_index()\n",
    "regime_percentages = (regime_counts / len(regimes)) * 100\n",
    "\n",
    "bars = axes[3].bar(regime_counts.index, regime_percentages, \n",
    "                  color=[regime_colors[i] for i in regime_counts.index], alpha=0.7)\n",
    "axes[3].set_title('Volatility Regime Distribution')\n",
    "axes[3].set_xlabel('Regime')\n",
    "axes[3].set_ylabel('Percentage of Time')\n",
    "axes[3].set_xticks([0, 1, 2])\n",
    "axes[3].set_xticklabels(['Low Vol', 'Medium Vol', 'High Vol'])\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, pct in zip(bars, regime_percentages):\n",
    "    height = bar.get_height()\n",
    "    axes[3].text(bar.get_x() + bar.get_width()/2., height + 0.5, f'{pct:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed regime analysis\n",
    "print(f\"\\nüîç REGIME ANALYSIS RESULTS:\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "persistence = regime_results['persistence']\n",
    "print(f\"Average Regime Persistence (days):\")\n",
    "print(f\"  Low Volatility:    {persistence[0]:.1f}\")\n",
    "print(f\"  Medium Volatility: {persistence[1]:.1f}\")  \n",
    "print(f\"  High Volatility:   {persistence[2]:.1f}\")\n",
    "\n",
    "print(f\"\\nRegime Distribution:\")\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = (count / len(regimes)) * 100\n",
    "    print(f\"  {regime_labels[regime]}: {count} days ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRegime Transitions: {len(regime_results['transitions'])} changes\")\n",
    "\n",
    "# Current regime\n",
    "current_regime = regimes.iloc[-1] if not pd.isna(regimes.iloc[-1]) else 1\n",
    "current_vol = vols['vol_20d'].iloc[-1]\n",
    "print(f\"\\nCurrent Status:\")\n",
    "print(f\"  Regime: {regime_labels[current_regime]}\")\n",
    "print(f\"  20-day Volatility: {current_vol:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Advanced Challenge Complete: Volatility regime detection implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbebb8c",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "**INSTRUCTOR NOTE**: Comprehensive review of Week 2 concepts and preparation for Week 3.\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Data Cleaning**: Professional-grade data validation and preprocessing\n",
    "2. **Basic Features**: Returns, moving averages, volatility measures\n",
    "3. **Technical Indicators**: RSI, MACD, Bollinger Bands, Stochastic, Williams %R, ATR\n",
    "4. **Advanced Analysis**: PCA vs Feature Importance comparison\n",
    "5. **Regime Detection**: Volatility regime classification and persistence analysis\n",
    "\n",
    "### Student Assessment Checklist:\n",
    "- [ ] Can implement robust data loading with error handling\n",
    "- [ ] Understands different return calculations (simple vs log)\n",
    "- [ ] Can calculate and interpret technical indicators\n",
    "- [ ] Understands when to use PCA vs feature selection\n",
    "- [ ] Can detect and analyze volatility regimes\n",
    "\n",
    "### Week 3 Preparation:\n",
    "- Model selection and hyperparameter tuning\n",
    "- Cross-validation for time series\n",
    "- Advanced ensemble methods\n",
    "- Risk metrics and portfolio optimization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
